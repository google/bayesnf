{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayesian Neural Fields for Spatiotemporal Prediction \u00a4 This is not an officially supported Google product. Spatially referenced time series (i.e., spatiotemporal) datasets are ubiquitous in scientific, engineering, and business-intelligence applications. This repository contains an implementation of the Bayesian Neural Field (BayesNF), a spatiotemporal modeling method that integrates hierarchical Bayesian inference for accurate uncertainty estimation with deep neural networks for high-capacity function approximation. BayesNF infer joint probability distributions over field values at arbitrary points in time and space, which makes the model suitable for many data-analysis tasks including spatial interpolation, temporal forecasting, and variography. Posterior inference is conducted using variationally learned surrogates trained via mini-batch stochastic gradient descent for handling large-scale data. The system is build on the JAX machine learning platform. The probabilistic model and inference algorithm are described in the following paper : Scalable spatiotemporal prediction with Bayesian neural fields . Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\u00f6ster, Rif A. Saurous, Matthew Hoffman. Nature Communications 15 , 7942 (2024). https://doi.org/10.1038/s41467-024-51477-5 @article { title = {Scalable Spatiotemporal Prediction with {Bayesian } Neural Fields}, authors = {Saad, Feras and Burnim, Jacob and Carroll, Colin and Patton, Brian and K\u00f6ster, Urs and Saurous, Rif A. and Hoffman, Matthew} journal = {Nature Communications}, volume = {15}, issue = {7942}, year = {2024}, doi = {10.1038/s41467-024-51477-5}, publisher = {Springer Nature}, } Installation \u00a4 bayesnf can be installed from the Python Package Index ( PyPI ) using: $ python -m pip install bayesnf The typical install time is 1 minute. This software is tested on Python 3.10 with a standard Debian GNU/Linux setup. The large-scale experiments in scripts/ were run using TPU v3-8 accelerators . To run BayesNF locally on medium to large-scale data, a GPU is required at minimum. Installation into a virtual environment is highly recommended, using the following steps: $ python -m venv pyenv $ source pyenv/bin/activate $ python -m pip install -U bayesnf The versions of dependencies will depend on the Python version. Github Actions tests the software using Python 3.10. If encountering any version issues, please refer to the following file for the versions of libraries used in the test suite: requirements.Python3.10.14.txt . These specific versions can be installed into the virtual environment using the following command: $ python -m pip install -r requirements.Python3.10.14.txt Documentation and Tutorials \u00a4 Please visit https://google.github.io/bayesnf Quick start \u00a4 # Load a dataframe with \"long\" format spatiotemporal data. df_train = pd . read_csv ( 'chickenpox.5.train.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) # Build a BayesianNeuralFieldEstimator model = BayesianNeuralFieldMAP ( width = 256 , depth = 2 , freq = 'W' , seasonality_periods = [ 'M' , 'Y' ], num_seasonal_harmonics = [ 2 , 10 ], feature_cols = [ 'datetime' , 'latitude' , 'longitude' ], target_col = 'chickenpox' , observation_model = 'NORMAL' , timetype = 'index' , standardize = [ 'latitude' , 'longitude' ], interactions = [( 0 , 1 ), ( 0 , 2 ), ( 1 , 2 )]) # Fit the model. model = model . fit ( df_train , seed = jax . random . PRNGKey ( 0 ), ensemble_size = ensemble_size , num_epochs = num_epochs ) # Make predictions of means and quantiles on test data. df_test = pd . read_csv ( 'chickenpox.5.test.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) yhat , yhat_quantiles = model . predict ( df_test , quantiles = ( 0.025 , 0.5 , 0.975 ))","title":"Home"},{"location":"#bayesian-neural-fields-for-spatiotemporal-prediction","text":"This is not an officially supported Google product. Spatially referenced time series (i.e., spatiotemporal) datasets are ubiquitous in scientific, engineering, and business-intelligence applications. This repository contains an implementation of the Bayesian Neural Field (BayesNF), a spatiotemporal modeling method that integrates hierarchical Bayesian inference for accurate uncertainty estimation with deep neural networks for high-capacity function approximation. BayesNF infer joint probability distributions over field values at arbitrary points in time and space, which makes the model suitable for many data-analysis tasks including spatial interpolation, temporal forecasting, and variography. Posterior inference is conducted using variationally learned surrogates trained via mini-batch stochastic gradient descent for handling large-scale data. The system is build on the JAX machine learning platform. The probabilistic model and inference algorithm are described in the following paper : Scalable spatiotemporal prediction with Bayesian neural fields . Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\u00f6ster, Rif A. Saurous, Matthew Hoffman. Nature Communications 15 , 7942 (2024). https://doi.org/10.1038/s41467-024-51477-5 @article { title = {Scalable Spatiotemporal Prediction with {Bayesian } Neural Fields}, authors = {Saad, Feras and Burnim, Jacob and Carroll, Colin and Patton, Brian and K\u00f6ster, Urs and Saurous, Rif A. and Hoffman, Matthew} journal = {Nature Communications}, volume = {15}, issue = {7942}, year = {2024}, doi = {10.1038/s41467-024-51477-5}, publisher = {Springer Nature}, }","title":"Bayesian Neural Fields for Spatiotemporal Prediction"},{"location":"#installation","text":"bayesnf can be installed from the Python Package Index ( PyPI ) using: $ python -m pip install bayesnf The typical install time is 1 minute. This software is tested on Python 3.10 with a standard Debian GNU/Linux setup. The large-scale experiments in scripts/ were run using TPU v3-8 accelerators . To run BayesNF locally on medium to large-scale data, a GPU is required at minimum. Installation into a virtual environment is highly recommended, using the following steps: $ python -m venv pyenv $ source pyenv/bin/activate $ python -m pip install -U bayesnf The versions of dependencies will depend on the Python version. Github Actions tests the software using Python 3.10. If encountering any version issues, please refer to the following file for the versions of libraries used in the test suite: requirements.Python3.10.14.txt . These specific versions can be installed into the virtual environment using the following command: $ python -m pip install -r requirements.Python3.10.14.txt","title":"Installation"},{"location":"#documentation-and-tutorials","text":"Please visit https://google.github.io/bayesnf","title":"Documentation and Tutorials"},{"location":"#quick-start","text":"# Load a dataframe with \"long\" format spatiotemporal data. df_train = pd . read_csv ( 'chickenpox.5.train.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) # Build a BayesianNeuralFieldEstimator model = BayesianNeuralFieldMAP ( width = 256 , depth = 2 , freq = 'W' , seasonality_periods = [ 'M' , 'Y' ], num_seasonal_harmonics = [ 2 , 10 ], feature_cols = [ 'datetime' , 'latitude' , 'longitude' ], target_col = 'chickenpox' , observation_model = 'NORMAL' , timetype = 'index' , standardize = [ 'latitude' , 'longitude' ], interactions = [( 0 , 1 ), ( 0 , 2 ), ( 1 , 2 )]) # Fit the model. model = model . fit ( df_train , seed = jax . random . PRNGKey ( 0 ), ensemble_size = ensemble_size , num_epochs = num_epochs ) # Make predictions of means and quantiles on test data. df_test = pd . read_csv ( 'chickenpox.5.test.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) yhat , yhat_quantiles = model . predict ( df_test , quantiles = ( 0.025 , 0.5 , 0.975 ))","title":"Quick start"},{"location":"api/BayesianNeuralFieldEstimator/","text":"BayesianNeuralFieldEstimator \u00a4 Base class for BayesNF estimators. This class should not be initialized directly, but rather one of the three subclasses that implement different model learning procedures: BayesianNeuralFieldVI , for ensembles of surrogate posteriors from variational inference. BayesianNeuralFieldMAP , for stochastic ensembles of maximum-a-posteriori estimates. BayesianNeuralFieldMLE , for stochastic ensembles of maximum likelihood estimates. All three classes share the same __init__ method described below. Source code in bayesnf/spatiotemporal.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 class BayesianNeuralFieldEstimator : \"\"\"Base class for BayesNF estimators. This class should not be initialized directly, but rather one of the three subclasses that implement different model learning procedures: - [BayesianNeuralFieldVI](BayesianNeuralFieldVI.md), for ensembles of surrogate posteriors from variational inference. - [BayesianNeuralFieldMAP](BayesianNeuralFieldMAP.md), for stochastic ensembles of maximum-a-posteriori estimates. - [BayesianNeuralFieldMLE](BayesianNeuralFieldMLE.md), for stochastic ensembles of maximum likelihood estimates. All three classes share the same `__init__` method described below. \"\"\" _ensemble_dims : int _prior_weight : float = 1.0 _scale_epochs_by_batch_size : bool = False def __init__ ( self , * , feature_cols : Sequence [ str ], target_col : str , seasonality_periods : Sequence [ float | str ] | None = None , num_seasonal_harmonics : Sequence [ int ] | None = None , fourier_degrees : Sequence [ float ] | None = None , interactions : Sequence [ tuple [ int , int ]] | None = None , freq : str | None = None , timetype : str = 'index' , depth : int = 2 , width : int = 512 , observation_model : str = 'NORMAL' , standardize : Sequence [ str ] | None = None , ): \"\"\"Shared initialization for subclasses of BayesianNeuralFieldEstimator. Args: feature_cols: Names of columns to use as features in the training data frame. The first entry denotes the name of the time variable, the remaining entries (if any) denote names of the spatial features. target_col: Name of the target column representing the spatial field. seasonality_periods: A list of numbers representing the seasonal frequencies of the data in the time domain. If timetype == 'index', then it is possible to specify numeric frequencies by using string short hands such as 'W', 'D', etc., which correspond to a valid Pandas frequency. See Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid string values. num_seasonal_harmonics: A list of seasonal harmonics, one for each entry in `seasonality_periods`. The number of seasonal harmonics (h) for a given seasonal period `p` must satisfy `h < p//2`. It is an error fir `len(num_seasonal_harmonics) != len(seasonality_periods)`. Should be used only if `timetype == 'index'`. fourier_degrees: A list of integer degrees for the Fourier features of the inputs. If given, must have the same length as `feature_cols`. interactions: A list of tuples of column indexes for the first-order interactions. For example `[(0,1), (1,2)]` creates two interaction features - `feature_cols[0] * feature_cols[1]` - `feature_cols[1] * feature_cols[2]` freq: A frequency string for the sampling rate at which the data is collected. See the Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid values. Should be used if and only if `timetype == 'index'`. timetype: Either `index` or `float`. If `index`, then the time column must be a `datetime` type and `freq` must be given. Otherwise, if `float`, then the time column must be `float`. depth: The number of hidden layers in the BayesNF architecture. width: The number of hidden units in each layer. observation_model: The aleatoric noise model for the observed data. The options are `NORMAL` (Gaussian noise), `NB` (negative binomial noise), or `ZNB` (zero-inflated negative binomial noise). standardize: List of columns that should be standardized. It is highly recommended to standardize `feature_cols[1:]`. It is an error if `features_cols[0]` (the time variable) is in `standardize`. \"\"\" self . num_seasonal_harmonics = num_seasonal_harmonics self . seasonality_periods = seasonality_periods self . observation_model = observation_model self . depth = depth self . width = width self . feature_cols = feature_cols self . target_col = target_col self . timetype = timetype self . freq = freq self . fourier_degrees = fourier_degrees self . standardize = standardize self . interactions = interactions self . losses_ = None self . params_ = None self . data_handler = SpatiotemporalDataHandler ( self . feature_cols , self . target_col , self . timetype , self . freq , standardize = self . standardize ) def _get_fourier_degrees ( self , batch_shape : tuple [ int , ... ]) -> np . ndarray : \"\"\"Set default fourier degrees, or verify shape is correct.\"\"\" if self . fourier_degrees is None : fourier_degrees = np . full ( batch_shape [ - 1 ], 5 , dtype = int ) else : fourier_degrees = np . atleast_1d ( self . fourier_degrees ) . astype ( int ) if fourier_degrees . shape [ - 1 ] != batch_shape [ - 1 ]: raise ValueError ( 'The length of fourier_degrees ( {} ) must match the ' 'input dimension dimension ( {} ).' . format ( fourier_degrees . shape [ - 1 ], batch_shape [ - 1 ] ) ) return fourier_degrees def _get_interactions ( self ) -> np . ndarray : \"\"\"Set default fourier degrees, or verify shape is correct.\"\"\" if self . interactions is None : interactions = np . zeros (( 0 , 2 ), dtype = int ) else : interactions = np . array ( self . interactions ) . astype ( int ) if np . ndim ( interactions ) != 2 or interactions . shape [ - 1 ] != 2 : raise ValueError ( 'The argument for `interactions` should be a 2-d array of integers ' 'of shape (N, 2), indicating the column indices to interact (the ' f ' passed shape was { interactions . shape } )' ) return interactions def _get_seasonality_periods ( self ): \"\"\"Return array of seasonal periods.\"\"\" if ( ( self . timetype == 'index' and self . freq is None ) or ( self . timetype == 'float' and self . freq is not None )): raise ValueError ( f 'Invalid { self . freq =} with { self . timetype =} .' ) if self . seasonality_periods is None : return np . zeros ( 0 ) if self . timetype == 'index' : return seasonalities_to_array ( self . seasonality_periods , self . freq ) if self . timetype == 'float' : return np . asarray ( self . seasonality_periods , dtype = float ) assert False , f 'Impossible { self . timetype =} .' def _get_num_seasonal_harmonics ( self ): \"\"\"Return array of seasonal harmonics per seasonal period.\"\"\" # Discrete time. if self . timetype == 'index' : return ( np . array ( self . num_seasonal_harmonics ) if self . num_seasonal_harmonics is not None else np . zeros ( 0 )) # Continuous time. if self . timetype == 'float' : if self . num_seasonal_harmonics is not None : raise ValueError ( f 'Cannot use num_seasonal_harmonics with { self . timetype =} .' ) # HACK: models.make_seasonal_frequencies assumes the data is discrete # time where each harmonic h is between 1, ..., p/2 and the harmonic # factors are np.arange(1, h + 1). Since our goal with continuous # time data is exactly 1 harmonic per seasonal factor, any h between # 0 and min(0.5, p/2) will work, as np.arange(1, 1+h) = [1] return np . fmin ( .5 , self . _get_seasonality_periods () / 2 ) assert False , f 'Impossible { self . timetype =} .' def _model_args ( self , batch_shape ): return { 'depth' : self . depth , 'input_scales' : self . data_handler . get_input_scales (), 'num_seasonal_harmonics' : self . _get_num_seasonal_harmonics (), 'seasonality_periods' : self . _get_seasonality_periods (), 'width' : self . width , 'init_x' : batch_shape , 'fourier_degrees' : self . _get_fourier_degrees ( batch_shape ), 'interactions' : self . _get_interactions (), } def predict ( self , table , quantiles = ( 0.5 ,), approximate_quantiles = False ): \"\"\"Make predictions of the target column at new times. Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. quantiles (Sequence[float]): The list of quantiles to compute. approximate_quantiles (bool): If `False,` uses Chandrupatla root finding to compute quantiles. If `True`, uses a heuristic approximation of the quantiles. Returns: means (np.ndarray): The predicted means from each particle in the learned ensemble. The shape is `(num_devices, ensemble_size // num_devices, len(table))` and can be flattened to a 2D array using `np.row_stack(means)`. Related https://github.com/google/bayesnf/issues/17 quantiles (List[np.ndarray]): A list of numpy arrays, one per requested quantile. The length of each array in the list is `len(table)`. \"\"\" test_data = self . data_handler . get_test ( table ) return inference . predict_bnf ( test_data , self . observation_model , params = self . params_ , model_args = self . _model_args ( test_data . shape ), quantiles = quantiles , ensemble_dims = self . _ensemble_dims , approximate_quantiles = approximate_quantiles , ) def fit ( self , table , seed ): \"\"\"Run inference given a training data `table` and `seed`. Cannot be directly called on `BayesianNeuralFieldEstimator`. Args: table (pandas.DataFrame): A pandas DataFrame representing the training data. It has the following requirements: - The columns of `table` should contain all `self.feature_cols` and the `self.target_col`. - The type of the \"time\" column (i.e., `self.feature_cols[0]`) should be `datetime`. To ensure this requirement holds, see [`pandas.to_datetime`]( https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html). The types of the remaining feature columns should be numeric. seed (jax.random.PRNGKey): The jax random key. \"\"\" raise NotImplementedError ( 'Should be implemented by subclass' ) def likelihood_model ( self , table : pd . DataFrame ) -> tfd . Distribution : \"\"\"Access the predictive distribution over new field values in `table`. NOTE: Must be called after [`fit`](). Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. Returns: A probability distribution representing the predictive distribution over `self.target_col` at the new field values in `table`. See [tfp.distributions.Distribution]( https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution) for the methods associated with this object. \"\"\" test_data = self . data_handler . get_test ( table ) mlp , mlp_template = inference . make_model ( ** self . _model_args ( test_data . shape )) for _ in range ( self . _ensemble_dims - 1 ): mlp . apply = jax . vmap ( mlp . apply , in_axes = ( 0 , None )) mlp . apply = jax . pmap ( mlp . apply , in_axes = ( 0 , None )) # This allows the likelihood to broadcast correctly with the batch of # predictions. params = self . params_ . _replace ( ** { # pytype: disable=attribute-error self . params_ . _fields [ i ]: self . params_ [ i ][ ... , jnp . newaxis ] # pytype: disable=unsupported-operands,attribute-error for i in range ( 3 )}) return models . make_likelihood_model ( params , jnp . array ( test_data ), mlp , mlp_template , self . observation_model ) __init__ \u00a4 __init__ ( * , feature_cols , target_col , seasonality_periods = None , num_seasonal_harmonics = None , fourier_degrees = None , interactions = None , freq = None , timetype = 'index' , depth = 2 , width = 512 , observation_model = 'NORMAL' , standardize = None ) Shared initialization for subclasses of BayesianNeuralFieldEstimator. PARAMETER DESCRIPTION feature_cols Names of columns to use as features in the training data frame. The first entry denotes the name of the time variable, the remaining entries (if any) denote names of the spatial features. TYPE: Sequence [ str ] target_col Name of the target column representing the spatial field. TYPE: str seasonality_periods A list of numbers representing the seasonal frequencies of the data in the time domain. If timetype == 'index', then it is possible to specify numeric frequencies by using string short hands such as 'W', 'D', etc., which correspond to a valid Pandas frequency. See Pandas Offset Aliases for valid string values. TYPE: Sequence [ float | str ] | None DEFAULT: None num_seasonal_harmonics A list of seasonal harmonics, one for each entry in seasonality_periods . The number of seasonal harmonics (h) for a given seasonal period p must satisfy h < p//2 . It is an error fir len(num_seasonal_harmonics) != len(seasonality_periods) . Should be used only if timetype == 'index' . TYPE: Sequence [ int ] | None DEFAULT: None fourier_degrees A list of integer degrees for the Fourier features of the inputs. If given, must have the same length as feature_cols . TYPE: Sequence [ float ] | None DEFAULT: None interactions A list of tuples of column indexes for the first-order interactions. For example [(0,1), (1,2)] creates two interaction features - feature_cols[0] * feature_cols[1] - feature_cols[1] * feature_cols[2] TYPE: Sequence [ tuple [ int , int ]] | None DEFAULT: None freq A frequency string for the sampling rate at which the data is collected. See the Pandas Offset Aliases for valid values. Should be used if and only if timetype == 'index' . TYPE: str | None DEFAULT: None timetype Either index or float . If index , then the time column must be a datetime type and freq must be given. Otherwise, if float , then the time column must be float . TYPE: str DEFAULT: 'index' depth The number of hidden layers in the BayesNF architecture. TYPE: int DEFAULT: 2 width The number of hidden units in each layer. TYPE: int DEFAULT: 512 observation_model The aleatoric noise model for the observed data. The options are NORMAL (Gaussian noise), NB (negative binomial noise), or ZNB (zero-inflated negative binomial noise). TYPE: str DEFAULT: 'NORMAL' standardize List of columns that should be standardized. It is highly recommended to standardize feature_cols[1:] . It is an error if features_cols[0] (the time variable) is in standardize . TYPE: Sequence [ str ] | None DEFAULT: None Source code in bayesnf/spatiotemporal.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def __init__ ( self , * , feature_cols : Sequence [ str ], target_col : str , seasonality_periods : Sequence [ float | str ] | None = None , num_seasonal_harmonics : Sequence [ int ] | None = None , fourier_degrees : Sequence [ float ] | None = None , interactions : Sequence [ tuple [ int , int ]] | None = None , freq : str | None = None , timetype : str = 'index' , depth : int = 2 , width : int = 512 , observation_model : str = 'NORMAL' , standardize : Sequence [ str ] | None = None , ): \"\"\"Shared initialization for subclasses of BayesianNeuralFieldEstimator. Args: feature_cols: Names of columns to use as features in the training data frame. The first entry denotes the name of the time variable, the remaining entries (if any) denote names of the spatial features. target_col: Name of the target column representing the spatial field. seasonality_periods: A list of numbers representing the seasonal frequencies of the data in the time domain. If timetype == 'index', then it is possible to specify numeric frequencies by using string short hands such as 'W', 'D', etc., which correspond to a valid Pandas frequency. See Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid string values. num_seasonal_harmonics: A list of seasonal harmonics, one for each entry in `seasonality_periods`. The number of seasonal harmonics (h) for a given seasonal period `p` must satisfy `h < p//2`. It is an error fir `len(num_seasonal_harmonics) != len(seasonality_periods)`. Should be used only if `timetype == 'index'`. fourier_degrees: A list of integer degrees for the Fourier features of the inputs. If given, must have the same length as `feature_cols`. interactions: A list of tuples of column indexes for the first-order interactions. For example `[(0,1), (1,2)]` creates two interaction features - `feature_cols[0] * feature_cols[1]` - `feature_cols[1] * feature_cols[2]` freq: A frequency string for the sampling rate at which the data is collected. See the Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid values. Should be used if and only if `timetype == 'index'`. timetype: Either `index` or `float`. If `index`, then the time column must be a `datetime` type and `freq` must be given. Otherwise, if `float`, then the time column must be `float`. depth: The number of hidden layers in the BayesNF architecture. width: The number of hidden units in each layer. observation_model: The aleatoric noise model for the observed data. The options are `NORMAL` (Gaussian noise), `NB` (negative binomial noise), or `ZNB` (zero-inflated negative binomial noise). standardize: List of columns that should be standardized. It is highly recommended to standardize `feature_cols[1:]`. It is an error if `features_cols[0]` (the time variable) is in `standardize`. \"\"\" self . num_seasonal_harmonics = num_seasonal_harmonics self . seasonality_periods = seasonality_periods self . observation_model = observation_model self . depth = depth self . width = width self . feature_cols = feature_cols self . target_col = target_col self . timetype = timetype self . freq = freq self . fourier_degrees = fourier_degrees self . standardize = standardize self . interactions = interactions self . losses_ = None self . params_ = None self . data_handler = SpatiotemporalDataHandler ( self . feature_cols , self . target_col , self . timetype , self . freq , standardize = self . standardize ) predict \u00a4 predict ( table , quantiles = ( 0.5 ), approximate_quantiles = False ) Make predictions of the target column at new times. PARAMETER DESCRIPTION table Field locations at which to make new predictions. Same as table in fit , except that self.target_col need not be in table . TYPE: DataFrame quantiles The list of quantiles to compute. TYPE: Sequence [ float ] DEFAULT: (0.5) approximate_quantiles If False, uses Chandrupatla root finding to compute quantiles. If True , uses a heuristic approximation of the quantiles. TYPE: bool DEFAULT: False RETURNS DESCRIPTION means The predicted means from each particle in the learned ensemble. The shape is (num_devices, ensemble_size // num_devices, len(table)) and can be flattened to a 2D array using np.row_stack(means) . Related https://github.com/google/bayesnf/issues/17 TYPE: ndarray quantiles A list of numpy arrays, one per requested quantile. The length of each array in the list is len(table) . TYPE: List [ ndarray ] Source code in bayesnf/spatiotemporal.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def predict ( self , table , quantiles = ( 0.5 ,), approximate_quantiles = False ): \"\"\"Make predictions of the target column at new times. Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. quantiles (Sequence[float]): The list of quantiles to compute. approximate_quantiles (bool): If `False,` uses Chandrupatla root finding to compute quantiles. If `True`, uses a heuristic approximation of the quantiles. Returns: means (np.ndarray): The predicted means from each particle in the learned ensemble. The shape is `(num_devices, ensemble_size // num_devices, len(table))` and can be flattened to a 2D array using `np.row_stack(means)`. Related https://github.com/google/bayesnf/issues/17 quantiles (List[np.ndarray]): A list of numpy arrays, one per requested quantile. The length of each array in the list is `len(table)`. \"\"\" test_data = self . data_handler . get_test ( table ) return inference . predict_bnf ( test_data , self . observation_model , params = self . params_ , model_args = self . _model_args ( test_data . shape ), quantiles = quantiles , ensemble_dims = self . _ensemble_dims , approximate_quantiles = approximate_quantiles , ) fit \u00a4 fit ( table , seed ) Run inference given a training data table and seed . Cannot be directly called on BayesianNeuralFieldEstimator . PARAMETER DESCRIPTION table A pandas DataFrame representing the training data. It has the following requirements: The columns of table should contain all self.feature_cols and the self.target_col . The type of the \"time\" column (i.e., self.feature_cols[0] ) should be datetime . To ensure this requirement holds, see pandas.to_datetime . The types of the remaining feature columns should be numeric. TYPE: DataFrame seed The jax random key. TYPE: PRNGKey Source code in bayesnf/spatiotemporal.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 def fit ( self , table , seed ): \"\"\"Run inference given a training data `table` and `seed`. Cannot be directly called on `BayesianNeuralFieldEstimator`. Args: table (pandas.DataFrame): A pandas DataFrame representing the training data. It has the following requirements: - The columns of `table` should contain all `self.feature_cols` and the `self.target_col`. - The type of the \"time\" column (i.e., `self.feature_cols[0]`) should be `datetime`. To ensure this requirement holds, see [`pandas.to_datetime`]( https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html). The types of the remaining feature columns should be numeric. seed (jax.random.PRNGKey): The jax random key. \"\"\" raise NotImplementedError ( 'Should be implemented by subclass' ) likelihood_model \u00a4 likelihood_model ( table ) Access the predictive distribution over new field values in table . NOTE: Must be called after fit . PARAMETER DESCRIPTION table Field locations at which to make new predictions. Same as table in fit , except that self.target_col need not be in table . TYPE: DataFrame RETURNS DESCRIPTION Distribution A probability distribution representing the predictive distribution over self.target_col at the new field values in table . See tfp.distributions.Distribution for the methods associated with this object. Source code in bayesnf/spatiotemporal.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def likelihood_model ( self , table : pd . DataFrame ) -> tfd . Distribution : \"\"\"Access the predictive distribution over new field values in `table`. NOTE: Must be called after [`fit`](). Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. Returns: A probability distribution representing the predictive distribution over `self.target_col` at the new field values in `table`. See [tfp.distributions.Distribution]( https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution) for the methods associated with this object. \"\"\" test_data = self . data_handler . get_test ( table ) mlp , mlp_template = inference . make_model ( ** self . _model_args ( test_data . shape )) for _ in range ( self . _ensemble_dims - 1 ): mlp . apply = jax . vmap ( mlp . apply , in_axes = ( 0 , None )) mlp . apply = jax . pmap ( mlp . apply , in_axes = ( 0 , None )) # This allows the likelihood to broadcast correctly with the batch of # predictions. params = self . params_ . _replace ( ** { # pytype: disable=attribute-error self . params_ . _fields [ i ]: self . params_ [ i ][ ... , jnp . newaxis ] # pytype: disable=unsupported-operands,attribute-error for i in range ( 3 )}) return models . make_likelihood_model ( params , jnp . array ( test_data ), mlp , mlp_template , self . observation_model )","title":"BayesianNeuralFieldEstimator"},{"location":"api/BayesianNeuralFieldEstimator/#bayesianneuralfieldestimator","text":"Base class for BayesNF estimators. This class should not be initialized directly, but rather one of the three subclasses that implement different model learning procedures: BayesianNeuralFieldVI , for ensembles of surrogate posteriors from variational inference. BayesianNeuralFieldMAP , for stochastic ensembles of maximum-a-posteriori estimates. BayesianNeuralFieldMLE , for stochastic ensembles of maximum likelihood estimates. All three classes share the same __init__ method described below. Source code in bayesnf/spatiotemporal.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 class BayesianNeuralFieldEstimator : \"\"\"Base class for BayesNF estimators. This class should not be initialized directly, but rather one of the three subclasses that implement different model learning procedures: - [BayesianNeuralFieldVI](BayesianNeuralFieldVI.md), for ensembles of surrogate posteriors from variational inference. - [BayesianNeuralFieldMAP](BayesianNeuralFieldMAP.md), for stochastic ensembles of maximum-a-posteriori estimates. - [BayesianNeuralFieldMLE](BayesianNeuralFieldMLE.md), for stochastic ensembles of maximum likelihood estimates. All three classes share the same `__init__` method described below. \"\"\" _ensemble_dims : int _prior_weight : float = 1.0 _scale_epochs_by_batch_size : bool = False def __init__ ( self , * , feature_cols : Sequence [ str ], target_col : str , seasonality_periods : Sequence [ float | str ] | None = None , num_seasonal_harmonics : Sequence [ int ] | None = None , fourier_degrees : Sequence [ float ] | None = None , interactions : Sequence [ tuple [ int , int ]] | None = None , freq : str | None = None , timetype : str = 'index' , depth : int = 2 , width : int = 512 , observation_model : str = 'NORMAL' , standardize : Sequence [ str ] | None = None , ): \"\"\"Shared initialization for subclasses of BayesianNeuralFieldEstimator. Args: feature_cols: Names of columns to use as features in the training data frame. The first entry denotes the name of the time variable, the remaining entries (if any) denote names of the spatial features. target_col: Name of the target column representing the spatial field. seasonality_periods: A list of numbers representing the seasonal frequencies of the data in the time domain. If timetype == 'index', then it is possible to specify numeric frequencies by using string short hands such as 'W', 'D', etc., which correspond to a valid Pandas frequency. See Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid string values. num_seasonal_harmonics: A list of seasonal harmonics, one for each entry in `seasonality_periods`. The number of seasonal harmonics (h) for a given seasonal period `p` must satisfy `h < p//2`. It is an error fir `len(num_seasonal_harmonics) != len(seasonality_periods)`. Should be used only if `timetype == 'index'`. fourier_degrees: A list of integer degrees for the Fourier features of the inputs. If given, must have the same length as `feature_cols`. interactions: A list of tuples of column indexes for the first-order interactions. For example `[(0,1), (1,2)]` creates two interaction features - `feature_cols[0] * feature_cols[1]` - `feature_cols[1] * feature_cols[2]` freq: A frequency string for the sampling rate at which the data is collected. See the Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid values. Should be used if and only if `timetype == 'index'`. timetype: Either `index` or `float`. If `index`, then the time column must be a `datetime` type and `freq` must be given. Otherwise, if `float`, then the time column must be `float`. depth: The number of hidden layers in the BayesNF architecture. width: The number of hidden units in each layer. observation_model: The aleatoric noise model for the observed data. The options are `NORMAL` (Gaussian noise), `NB` (negative binomial noise), or `ZNB` (zero-inflated negative binomial noise). standardize: List of columns that should be standardized. It is highly recommended to standardize `feature_cols[1:]`. It is an error if `features_cols[0]` (the time variable) is in `standardize`. \"\"\" self . num_seasonal_harmonics = num_seasonal_harmonics self . seasonality_periods = seasonality_periods self . observation_model = observation_model self . depth = depth self . width = width self . feature_cols = feature_cols self . target_col = target_col self . timetype = timetype self . freq = freq self . fourier_degrees = fourier_degrees self . standardize = standardize self . interactions = interactions self . losses_ = None self . params_ = None self . data_handler = SpatiotemporalDataHandler ( self . feature_cols , self . target_col , self . timetype , self . freq , standardize = self . standardize ) def _get_fourier_degrees ( self , batch_shape : tuple [ int , ... ]) -> np . ndarray : \"\"\"Set default fourier degrees, or verify shape is correct.\"\"\" if self . fourier_degrees is None : fourier_degrees = np . full ( batch_shape [ - 1 ], 5 , dtype = int ) else : fourier_degrees = np . atleast_1d ( self . fourier_degrees ) . astype ( int ) if fourier_degrees . shape [ - 1 ] != batch_shape [ - 1 ]: raise ValueError ( 'The length of fourier_degrees ( {} ) must match the ' 'input dimension dimension ( {} ).' . format ( fourier_degrees . shape [ - 1 ], batch_shape [ - 1 ] ) ) return fourier_degrees def _get_interactions ( self ) -> np . ndarray : \"\"\"Set default fourier degrees, or verify shape is correct.\"\"\" if self . interactions is None : interactions = np . zeros (( 0 , 2 ), dtype = int ) else : interactions = np . array ( self . interactions ) . astype ( int ) if np . ndim ( interactions ) != 2 or interactions . shape [ - 1 ] != 2 : raise ValueError ( 'The argument for `interactions` should be a 2-d array of integers ' 'of shape (N, 2), indicating the column indices to interact (the ' f ' passed shape was { interactions . shape } )' ) return interactions def _get_seasonality_periods ( self ): \"\"\"Return array of seasonal periods.\"\"\" if ( ( self . timetype == 'index' and self . freq is None ) or ( self . timetype == 'float' and self . freq is not None )): raise ValueError ( f 'Invalid { self . freq =} with { self . timetype =} .' ) if self . seasonality_periods is None : return np . zeros ( 0 ) if self . timetype == 'index' : return seasonalities_to_array ( self . seasonality_periods , self . freq ) if self . timetype == 'float' : return np . asarray ( self . seasonality_periods , dtype = float ) assert False , f 'Impossible { self . timetype =} .' def _get_num_seasonal_harmonics ( self ): \"\"\"Return array of seasonal harmonics per seasonal period.\"\"\" # Discrete time. if self . timetype == 'index' : return ( np . array ( self . num_seasonal_harmonics ) if self . num_seasonal_harmonics is not None else np . zeros ( 0 )) # Continuous time. if self . timetype == 'float' : if self . num_seasonal_harmonics is not None : raise ValueError ( f 'Cannot use num_seasonal_harmonics with { self . timetype =} .' ) # HACK: models.make_seasonal_frequencies assumes the data is discrete # time where each harmonic h is between 1, ..., p/2 and the harmonic # factors are np.arange(1, h + 1). Since our goal with continuous # time data is exactly 1 harmonic per seasonal factor, any h between # 0 and min(0.5, p/2) will work, as np.arange(1, 1+h) = [1] return np . fmin ( .5 , self . _get_seasonality_periods () / 2 ) assert False , f 'Impossible { self . timetype =} .' def _model_args ( self , batch_shape ): return { 'depth' : self . depth , 'input_scales' : self . data_handler . get_input_scales (), 'num_seasonal_harmonics' : self . _get_num_seasonal_harmonics (), 'seasonality_periods' : self . _get_seasonality_periods (), 'width' : self . width , 'init_x' : batch_shape , 'fourier_degrees' : self . _get_fourier_degrees ( batch_shape ), 'interactions' : self . _get_interactions (), } def predict ( self , table , quantiles = ( 0.5 ,), approximate_quantiles = False ): \"\"\"Make predictions of the target column at new times. Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. quantiles (Sequence[float]): The list of quantiles to compute. approximate_quantiles (bool): If `False,` uses Chandrupatla root finding to compute quantiles. If `True`, uses a heuristic approximation of the quantiles. Returns: means (np.ndarray): The predicted means from each particle in the learned ensemble. The shape is `(num_devices, ensemble_size // num_devices, len(table))` and can be flattened to a 2D array using `np.row_stack(means)`. Related https://github.com/google/bayesnf/issues/17 quantiles (List[np.ndarray]): A list of numpy arrays, one per requested quantile. The length of each array in the list is `len(table)`. \"\"\" test_data = self . data_handler . get_test ( table ) return inference . predict_bnf ( test_data , self . observation_model , params = self . params_ , model_args = self . _model_args ( test_data . shape ), quantiles = quantiles , ensemble_dims = self . _ensemble_dims , approximate_quantiles = approximate_quantiles , ) def fit ( self , table , seed ): \"\"\"Run inference given a training data `table` and `seed`. Cannot be directly called on `BayesianNeuralFieldEstimator`. Args: table (pandas.DataFrame): A pandas DataFrame representing the training data. It has the following requirements: - The columns of `table` should contain all `self.feature_cols` and the `self.target_col`. - The type of the \"time\" column (i.e., `self.feature_cols[0]`) should be `datetime`. To ensure this requirement holds, see [`pandas.to_datetime`]( https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html). The types of the remaining feature columns should be numeric. seed (jax.random.PRNGKey): The jax random key. \"\"\" raise NotImplementedError ( 'Should be implemented by subclass' ) def likelihood_model ( self , table : pd . DataFrame ) -> tfd . Distribution : \"\"\"Access the predictive distribution over new field values in `table`. NOTE: Must be called after [`fit`](). Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. Returns: A probability distribution representing the predictive distribution over `self.target_col` at the new field values in `table`. See [tfp.distributions.Distribution]( https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution) for the methods associated with this object. \"\"\" test_data = self . data_handler . get_test ( table ) mlp , mlp_template = inference . make_model ( ** self . _model_args ( test_data . shape )) for _ in range ( self . _ensemble_dims - 1 ): mlp . apply = jax . vmap ( mlp . apply , in_axes = ( 0 , None )) mlp . apply = jax . pmap ( mlp . apply , in_axes = ( 0 , None )) # This allows the likelihood to broadcast correctly with the batch of # predictions. params = self . params_ . _replace ( ** { # pytype: disable=attribute-error self . params_ . _fields [ i ]: self . params_ [ i ][ ... , jnp . newaxis ] # pytype: disable=unsupported-operands,attribute-error for i in range ( 3 )}) return models . make_likelihood_model ( params , jnp . array ( test_data ), mlp , mlp_template , self . observation_model )","title":"BayesianNeuralFieldEstimator"},{"location":"api/BayesianNeuralFieldEstimator/#bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.__init__","text":"__init__ ( * , feature_cols , target_col , seasonality_periods = None , num_seasonal_harmonics = None , fourier_degrees = None , interactions = None , freq = None , timetype = 'index' , depth = 2 , width = 512 , observation_model = 'NORMAL' , standardize = None ) Shared initialization for subclasses of BayesianNeuralFieldEstimator. PARAMETER DESCRIPTION feature_cols Names of columns to use as features in the training data frame. The first entry denotes the name of the time variable, the remaining entries (if any) denote names of the spatial features. TYPE: Sequence [ str ] target_col Name of the target column representing the spatial field. TYPE: str seasonality_periods A list of numbers representing the seasonal frequencies of the data in the time domain. If timetype == 'index', then it is possible to specify numeric frequencies by using string short hands such as 'W', 'D', etc., which correspond to a valid Pandas frequency. See Pandas Offset Aliases for valid string values. TYPE: Sequence [ float | str ] | None DEFAULT: None num_seasonal_harmonics A list of seasonal harmonics, one for each entry in seasonality_periods . The number of seasonal harmonics (h) for a given seasonal period p must satisfy h < p//2 . It is an error fir len(num_seasonal_harmonics) != len(seasonality_periods) . Should be used only if timetype == 'index' . TYPE: Sequence [ int ] | None DEFAULT: None fourier_degrees A list of integer degrees for the Fourier features of the inputs. If given, must have the same length as feature_cols . TYPE: Sequence [ float ] | None DEFAULT: None interactions A list of tuples of column indexes for the first-order interactions. For example [(0,1), (1,2)] creates two interaction features - feature_cols[0] * feature_cols[1] - feature_cols[1] * feature_cols[2] TYPE: Sequence [ tuple [ int , int ]] | None DEFAULT: None freq A frequency string for the sampling rate at which the data is collected. See the Pandas Offset Aliases for valid values. Should be used if and only if timetype == 'index' . TYPE: str | None DEFAULT: None timetype Either index or float . If index , then the time column must be a datetime type and freq must be given. Otherwise, if float , then the time column must be float . TYPE: str DEFAULT: 'index' depth The number of hidden layers in the BayesNF architecture. TYPE: int DEFAULT: 2 width The number of hidden units in each layer. TYPE: int DEFAULT: 512 observation_model The aleatoric noise model for the observed data. The options are NORMAL (Gaussian noise), NB (negative binomial noise), or ZNB (zero-inflated negative binomial noise). TYPE: str DEFAULT: 'NORMAL' standardize List of columns that should be standardized. It is highly recommended to standardize feature_cols[1:] . It is an error if features_cols[0] (the time variable) is in standardize . TYPE: Sequence [ str ] | None DEFAULT: None Source code in bayesnf/spatiotemporal.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def __init__ ( self , * , feature_cols : Sequence [ str ], target_col : str , seasonality_periods : Sequence [ float | str ] | None = None , num_seasonal_harmonics : Sequence [ int ] | None = None , fourier_degrees : Sequence [ float ] | None = None , interactions : Sequence [ tuple [ int , int ]] | None = None , freq : str | None = None , timetype : str = 'index' , depth : int = 2 , width : int = 512 , observation_model : str = 'NORMAL' , standardize : Sequence [ str ] | None = None , ): \"\"\"Shared initialization for subclasses of BayesianNeuralFieldEstimator. Args: feature_cols: Names of columns to use as features in the training data frame. The first entry denotes the name of the time variable, the remaining entries (if any) denote names of the spatial features. target_col: Name of the target column representing the spatial field. seasonality_periods: A list of numbers representing the seasonal frequencies of the data in the time domain. If timetype == 'index', then it is possible to specify numeric frequencies by using string short hands such as 'W', 'D', etc., which correspond to a valid Pandas frequency. See Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid string values. num_seasonal_harmonics: A list of seasonal harmonics, one for each entry in `seasonality_periods`. The number of seasonal harmonics (h) for a given seasonal period `p` must satisfy `h < p//2`. It is an error fir `len(num_seasonal_harmonics) != len(seasonality_periods)`. Should be used only if `timetype == 'index'`. fourier_degrees: A list of integer degrees for the Fourier features of the inputs. If given, must have the same length as `feature_cols`. interactions: A list of tuples of column indexes for the first-order interactions. For example `[(0,1), (1,2)]` creates two interaction features - `feature_cols[0] * feature_cols[1]` - `feature_cols[1] * feature_cols[2]` freq: A frequency string for the sampling rate at which the data is collected. See the Pandas [Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for valid values. Should be used if and only if `timetype == 'index'`. timetype: Either `index` or `float`. If `index`, then the time column must be a `datetime` type and `freq` must be given. Otherwise, if `float`, then the time column must be `float`. depth: The number of hidden layers in the BayesNF architecture. width: The number of hidden units in each layer. observation_model: The aleatoric noise model for the observed data. The options are `NORMAL` (Gaussian noise), `NB` (negative binomial noise), or `ZNB` (zero-inflated negative binomial noise). standardize: List of columns that should be standardized. It is highly recommended to standardize `feature_cols[1:]`. It is an error if `features_cols[0]` (the time variable) is in `standardize`. \"\"\" self . num_seasonal_harmonics = num_seasonal_harmonics self . seasonality_periods = seasonality_periods self . observation_model = observation_model self . depth = depth self . width = width self . feature_cols = feature_cols self . target_col = target_col self . timetype = timetype self . freq = freq self . fourier_degrees = fourier_degrees self . standardize = standardize self . interactions = interactions self . losses_ = None self . params_ = None self . data_handler = SpatiotemporalDataHandler ( self . feature_cols , self . target_col , self . timetype , self . freq , standardize = self . standardize )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;__init__"},{"location":"api/BayesianNeuralFieldEstimator/#bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.predict","text":"predict ( table , quantiles = ( 0.5 ), approximate_quantiles = False ) Make predictions of the target column at new times. PARAMETER DESCRIPTION table Field locations at which to make new predictions. Same as table in fit , except that self.target_col need not be in table . TYPE: DataFrame quantiles The list of quantiles to compute. TYPE: Sequence [ float ] DEFAULT: (0.5) approximate_quantiles If False, uses Chandrupatla root finding to compute quantiles. If True , uses a heuristic approximation of the quantiles. TYPE: bool DEFAULT: False RETURNS DESCRIPTION means The predicted means from each particle in the learned ensemble. The shape is (num_devices, ensemble_size // num_devices, len(table)) and can be flattened to a 2D array using np.row_stack(means) . Related https://github.com/google/bayesnf/issues/17 TYPE: ndarray quantiles A list of numpy arrays, one per requested quantile. The length of each array in the list is len(table) . TYPE: List [ ndarray ] Source code in bayesnf/spatiotemporal.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 def predict ( self , table , quantiles = ( 0.5 ,), approximate_quantiles = False ): \"\"\"Make predictions of the target column at new times. Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. quantiles (Sequence[float]): The list of quantiles to compute. approximate_quantiles (bool): If `False,` uses Chandrupatla root finding to compute quantiles. If `True`, uses a heuristic approximation of the quantiles. Returns: means (np.ndarray): The predicted means from each particle in the learned ensemble. The shape is `(num_devices, ensemble_size // num_devices, len(table))` and can be flattened to a 2D array using `np.row_stack(means)`. Related https://github.com/google/bayesnf/issues/17 quantiles (List[np.ndarray]): A list of numpy arrays, one per requested quantile. The length of each array in the list is `len(table)`. \"\"\" test_data = self . data_handler . get_test ( table ) return inference . predict_bnf ( test_data , self . observation_model , params = self . params_ , model_args = self . _model_args ( test_data . shape ), quantiles = quantiles , ensemble_dims = self . _ensemble_dims , approximate_quantiles = approximate_quantiles , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;predict"},{"location":"api/BayesianNeuralFieldEstimator/#bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit","text":"fit ( table , seed ) Run inference given a training data table and seed . Cannot be directly called on BayesianNeuralFieldEstimator . PARAMETER DESCRIPTION table A pandas DataFrame representing the training data. It has the following requirements: The columns of table should contain all self.feature_cols and the self.target_col . The type of the \"time\" column (i.e., self.feature_cols[0] ) should be datetime . To ensure this requirement holds, see pandas.to_datetime . The types of the remaining feature columns should be numeric. TYPE: DataFrame seed The jax random key. TYPE: PRNGKey Source code in bayesnf/spatiotemporal.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 def fit ( self , table , seed ): \"\"\"Run inference given a training data `table` and `seed`. Cannot be directly called on `BayesianNeuralFieldEstimator`. Args: table (pandas.DataFrame): A pandas DataFrame representing the training data. It has the following requirements: - The columns of `table` should contain all `self.feature_cols` and the `self.target_col`. - The type of the \"time\" column (i.e., `self.feature_cols[0]`) should be `datetime`. To ensure this requirement holds, see [`pandas.to_datetime`]( https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html). The types of the remaining feature columns should be numeric. seed (jax.random.PRNGKey): The jax random key. \"\"\" raise NotImplementedError ( 'Should be implemented by subclass' )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;fit"},{"location":"api/BayesianNeuralFieldEstimator/#bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.likelihood_model","text":"likelihood_model ( table ) Access the predictive distribution over new field values in table . NOTE: Must be called after fit . PARAMETER DESCRIPTION table Field locations at which to make new predictions. Same as table in fit , except that self.target_col need not be in table . TYPE: DataFrame RETURNS DESCRIPTION Distribution A probability distribution representing the predictive distribution over self.target_col at the new field values in table . See tfp.distributions.Distribution for the methods associated with this object. Source code in bayesnf/spatiotemporal.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def likelihood_model ( self , table : pd . DataFrame ) -> tfd . Distribution : \"\"\"Access the predictive distribution over new field values in `table`. NOTE: Must be called after [`fit`](). Args: table (pandas.DataFrame): Field locations at which to make new predictions. Same as `table` in [`fit`](), except that `self.target_col` need not be in `table`. Returns: A probability distribution representing the predictive distribution over `self.target_col` at the new field values in `table`. See [tfp.distributions.Distribution]( https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution) for the methods associated with this object. \"\"\" test_data = self . data_handler . get_test ( table ) mlp , mlp_template = inference . make_model ( ** self . _model_args ( test_data . shape )) for _ in range ( self . _ensemble_dims - 1 ): mlp . apply = jax . vmap ( mlp . apply , in_axes = ( 0 , None )) mlp . apply = jax . pmap ( mlp . apply , in_axes = ( 0 , None )) # This allows the likelihood to broadcast correctly with the batch of # predictions. params = self . params_ . _replace ( ** { # pytype: disable=attribute-error self . params_ . _fields [ i ]: self . params_ [ i ][ ... , jnp . newaxis ] # pytype: disable=unsupported-operands,attribute-error for i in range ( 3 )}) return models . make_likelihood_model ( params , jnp . array ( test_data ), mlp , mlp_template , self . observation_model )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;likelihood_model"},{"location":"api/BayesianNeuralFieldMAP/","text":"BayesianNeuralFieldMAP \u00a4 Bases: BayesianNeuralFieldEstimator Fits models using stochastic ensembles of maximum-a-posteriori estimates. Implementation of BayesianNeuralFieldEstimator . Source code in bayesnf/spatiotemporal.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 class BayesianNeuralFieldMAP ( BayesianNeuralFieldEstimator ): \"\"\"Fits models using stochastic ensembles of maximum-a-posteriori estimates. Implementation of [BayesianNeuralFieldEstimator](BayesianNeuralFieldEstimator.md). \"\"\" _ensemble_dims = 2 def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.005 , num_epochs = 5_000 , batch_size = None , num_splits = 1 , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic MAP ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles in the ensemble. It currently an error if `ensemble_size < jax.device_count`, but will be fixed in https://github.com/google/bayesnf/issues/28. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. batch_size (None | int): Batch size for SGD. Default is `None`, meaning full-batch. Each epoch will perform `len(table) // batch_size` SGD updates. num_splits (int): Number of splits over the data to run training. Defaults to 1, meaning there are no splits. Returns: Instance of `self`. \"\"\" if ensemble_size < jax . device_count (): raise ValueError ( 'ensemble_size cannot be smaller than device_count. ' 'https://github.com/google/bayesnf/issues/28.' ) train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) self . params_ , self . losses_ = inference . fit_map ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , num_particles = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , prior_weight = self . _prior_weight , batch_size = batch_size , num_splits = num_splits ) return self fit \u00a4 fit ( table , seed , ensemble_size = 16 , learning_rate = 0.005 , num_epochs = 5000 , batch_size = None , num_splits = 1 ) Run inference using stochastic MAP ensembles. PARAMETER DESCRIPTION table See documentation of table in the base class. TYPE: DataFrame seed The jax random key. TYPE: PRNGKey ensemble_size Number of particles in the ensemble. It currently an error if ensemble_size < jax.device_count , but will be fixed in https://github.com/google/bayesnf/issues/28. TYPE: int DEFAULT: 16 learning_rate Learning rate for SGD. TYPE: float DEFAULT: 0.005 num_epochs Number of full epochs through the training data. TYPE: int DEFAULT: 5000 batch_size Batch size for SGD. Default is None , meaning full-batch. Each epoch will perform len(table) // batch_size SGD updates. TYPE: None | int DEFAULT: None num_splits Number of splits over the data to run training. Defaults to 1, meaning there are no splits. TYPE: int DEFAULT: 1 RETURNS DESCRIPTION BayesianNeuralFieldEstimator Instance of self . Source code in bayesnf/spatiotemporal.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.005 , num_epochs = 5_000 , batch_size = None , num_splits = 1 , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic MAP ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles in the ensemble. It currently an error if `ensemble_size < jax.device_count`, but will be fixed in https://github.com/google/bayesnf/issues/28. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. batch_size (None | int): Batch size for SGD. Default is `None`, meaning full-batch. Each epoch will perform `len(table) // batch_size` SGD updates. num_splits (int): Number of splits over the data to run training. Defaults to 1, meaning there are no splits. Returns: Instance of `self`. \"\"\" if ensemble_size < jax . device_count (): raise ValueError ( 'ensemble_size cannot be smaller than device_count. ' 'https://github.com/google/bayesnf/issues/28.' ) train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) self . params_ , self . losses_ = inference . fit_map ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , num_particles = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , prior_weight = self . _prior_weight , batch_size = batch_size , num_splits = num_splits ) return self","title":"BayesianNeuralFieldMAP"},{"location":"api/BayesianNeuralFieldMAP/#bayesianneuralfieldmap","text":"Bases: BayesianNeuralFieldEstimator Fits models using stochastic ensembles of maximum-a-posteriori estimates. Implementation of BayesianNeuralFieldEstimator . Source code in bayesnf/spatiotemporal.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 class BayesianNeuralFieldMAP ( BayesianNeuralFieldEstimator ): \"\"\"Fits models using stochastic ensembles of maximum-a-posteriori estimates. Implementation of [BayesianNeuralFieldEstimator](BayesianNeuralFieldEstimator.md). \"\"\" _ensemble_dims = 2 def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.005 , num_epochs = 5_000 , batch_size = None , num_splits = 1 , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic MAP ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles in the ensemble. It currently an error if `ensemble_size < jax.device_count`, but will be fixed in https://github.com/google/bayesnf/issues/28. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. batch_size (None | int): Batch size for SGD. Default is `None`, meaning full-batch. Each epoch will perform `len(table) // batch_size` SGD updates. num_splits (int): Number of splits over the data to run training. Defaults to 1, meaning there are no splits. Returns: Instance of `self`. \"\"\" if ensemble_size < jax . device_count (): raise ValueError ( 'ensemble_size cannot be smaller than device_count. ' 'https://github.com/google/bayesnf/issues/28.' ) train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) self . params_ , self . losses_ = inference . fit_map ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , num_particles = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , prior_weight = self . _prior_weight , batch_size = batch_size , num_splits = num_splits ) return self","title":"BayesianNeuralFieldMAP"},{"location":"api/BayesianNeuralFieldMAP/#bayesnf.spatiotemporal.BayesianNeuralFieldMAP.fit","text":"fit ( table , seed , ensemble_size = 16 , learning_rate = 0.005 , num_epochs = 5000 , batch_size = None , num_splits = 1 ) Run inference using stochastic MAP ensembles. PARAMETER DESCRIPTION table See documentation of table in the base class. TYPE: DataFrame seed The jax random key. TYPE: PRNGKey ensemble_size Number of particles in the ensemble. It currently an error if ensemble_size < jax.device_count , but will be fixed in https://github.com/google/bayesnf/issues/28. TYPE: int DEFAULT: 16 learning_rate Learning rate for SGD. TYPE: float DEFAULT: 0.005 num_epochs Number of full epochs through the training data. TYPE: int DEFAULT: 5000 batch_size Batch size for SGD. Default is None , meaning full-batch. Each epoch will perform len(table) // batch_size SGD updates. TYPE: None | int DEFAULT: None num_splits Number of splits over the data to run training. Defaults to 1, meaning there are no splits. TYPE: int DEFAULT: 1 RETURNS DESCRIPTION BayesianNeuralFieldEstimator Instance of self . Source code in bayesnf/spatiotemporal.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.005 , num_epochs = 5_000 , batch_size = None , num_splits = 1 , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic MAP ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles in the ensemble. It currently an error if `ensemble_size < jax.device_count`, but will be fixed in https://github.com/google/bayesnf/issues/28. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. batch_size (None | int): Batch size for SGD. Default is `None`, meaning full-batch. Each epoch will perform `len(table) // batch_size` SGD updates. num_splits (int): Number of splits over the data to run training. Defaults to 1, meaning there are no splits. Returns: Instance of `self`. \"\"\" if ensemble_size < jax . device_count (): raise ValueError ( 'ensemble_size cannot be smaller than device_count. ' 'https://github.com/google/bayesnf/issues/28.' ) train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) self . params_ , self . losses_ = inference . fit_map ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , num_particles = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , prior_weight = self . _prior_weight , batch_size = batch_size , num_splits = num_splits ) return self","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;fit"},{"location":"api/BayesianNeuralFieldMLE/","text":"BayesianNeuralFieldMLE \u00a4 Bases: BayesianNeuralFieldMAP Fits models using stochastic ensembles of maximum likelihood estimates. Implementation of BayesianNeuralFieldEstimator . Source code in bayesnf/spatiotemporal.py 544 545 546 547 548 549 550 551 class BayesianNeuralFieldMLE ( BayesianNeuralFieldMAP ): \"\"\"Fits models using stochastic ensembles of maximum likelihood estimates. Implementation of [BayesianNeuralFieldEstimator](BayesianNeuralFieldEstimator.md). \"\"\" _prior_weight = 0.0","title":"BayesianNeuralFieldMLE"},{"location":"api/BayesianNeuralFieldMLE/#bayesianneuralfieldmle","text":"Bases: BayesianNeuralFieldMAP Fits models using stochastic ensembles of maximum likelihood estimates. Implementation of BayesianNeuralFieldEstimator . Source code in bayesnf/spatiotemporal.py 544 545 546 547 548 549 550 551 class BayesianNeuralFieldMLE ( BayesianNeuralFieldMAP ): \"\"\"Fits models using stochastic ensembles of maximum likelihood estimates. Implementation of [BayesianNeuralFieldEstimator](BayesianNeuralFieldEstimator.md). \"\"\" _prior_weight = 0.0","title":"BayesianNeuralFieldMLE"},{"location":"api/BayesianNeuralFieldVI/","text":"BayesianNeuralFieldVI \u00a4 Bases: BayesianNeuralFieldEstimator Fits models using stochastic ensembles of surrogate posteriors from VI. Implementation of BayesianNeuralFieldEstimator using variational inference (VI). Source code in bayesnf/spatiotemporal.py 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 class BayesianNeuralFieldVI ( BayesianNeuralFieldEstimator ): \"\"\"Fits models using stochastic ensembles of surrogate posteriors from VI. Implementation of [BayesianNeuralFieldEstimator](BayesianNeuralFieldEstimator.md) using variational inference (VI). \"\"\" _ensemble_dims = 3 _scale_epochs_by_batch_size = True def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.01 , num_epochs = 1_000 , sample_size_posterior = 30 , sample_size_divergence = 5 , kl_weight = 0.1 , batch_size = None , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic variational inference ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles (i.e., surrogate posteriors) in the ensemble, **per device**. The available devices can be found via `jax.devices()`. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. sample_size_posterior (int): Number of samples of \"posterior\" model parameters draw from each surrogate posterior when making predictions. sample_size_divergence (int): number of Monte Carlo samples to use in estimating the variational divergence. Larger values may stabilize the optimization, but at higher cost per step in time and memory. See [`tfp.vi.fit_surrogate_posterior_stateless`]( https://www.tensorflow.org/probability/api_docs/python/tfp/vi/fit_surrogate_posterior_stateless) for further details. kl_weight (float): Weighting of the KL divergence term in VI. The goal is to find a surrogate posterior `q(z)` that maximizes a version of the ELBO with the `KL(surrogate posterior || prior)` term scaled by `kl_weight` E_z~q [log p(x|z)] - kl_weight * KL(q || p) Reference > Weight Uncertainty in Neural Network > Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra. > Proceedings of the 32nd International Conference on Machine Learning. > PMLR 37:1613-1622, 2015. > <https://proceedings.mlr.press/v37/blundell15> batch_size (None | int): If specified, the log probability in each step of variational inference is computed on a batch of this size. Default is `None`, meaning full-batch. Returns: Instance of self. \"\"\" train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) _ , self . losses_ , self . params_ = inference . fit_vi ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , ensemble_size = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , sample_size_posterior = sample_size_posterior , sample_size_divergence = sample_size_divergence , kl_weight = kl_weight , batch_size = batch_size , ) return self fit \u00a4 fit ( table , seed , ensemble_size = 16 , learning_rate = 0.01 , num_epochs = 1000 , sample_size_posterior = 30 , sample_size_divergence = 5 , kl_weight = 0.1 , batch_size = None ) Run inference using stochastic variational inference ensembles. PARAMETER DESCRIPTION table See documentation of table in the base class. TYPE: DataFrame seed The jax random key. TYPE: PRNGKey ensemble_size Number of particles (i.e., surrogate posteriors) in the ensemble, per device . The available devices can be found via jax.devices() . TYPE: int DEFAULT: 16 learning_rate Learning rate for SGD. TYPE: float DEFAULT: 0.01 num_epochs Number of full epochs through the training data. TYPE: int DEFAULT: 1000 sample_size_posterior Number of samples of \"posterior\" model parameters draw from each surrogate posterior when making predictions. TYPE: int DEFAULT: 30 sample_size_divergence number of Monte Carlo samples to use in estimating the variational divergence. Larger values may stabilize the optimization, but at higher cost per step in time and memory. See tfp.vi.fit_surrogate_posterior_stateless for further details. TYPE: int DEFAULT: 5 kl_weight Weighting of the KL divergence term in VI. The goal is to find a surrogate posterior q(z) that maximizes a version of the ELBO with the KL(surrogate posterior || prior) term scaled by kl_weight E_z~q [log p(x|z)] - kl_weight * KL(q || p) Reference Weight Uncertainty in Neural Network Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra. Proceedings of the 32nd International Conference on Machine Learning. PMLR 37:1613-1622, 2015. https://proceedings.mlr.press/v37/blundell15 TYPE: float DEFAULT: 0.1 batch_size If specified, the log probability in each step of variational inference is computed on a batch of this size. Default is None , meaning full-batch. TYPE: None | int DEFAULT: None RETURNS DESCRIPTION BayesianNeuralFieldEstimator Instance of self. Source code in bayesnf/spatiotemporal.py 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.01 , num_epochs = 1_000 , sample_size_posterior = 30 , sample_size_divergence = 5 , kl_weight = 0.1 , batch_size = None , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic variational inference ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles (i.e., surrogate posteriors) in the ensemble, **per device**. The available devices can be found via `jax.devices()`. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. sample_size_posterior (int): Number of samples of \"posterior\" model parameters draw from each surrogate posterior when making predictions. sample_size_divergence (int): number of Monte Carlo samples to use in estimating the variational divergence. Larger values may stabilize the optimization, but at higher cost per step in time and memory. See [`tfp.vi.fit_surrogate_posterior_stateless`]( https://www.tensorflow.org/probability/api_docs/python/tfp/vi/fit_surrogate_posterior_stateless) for further details. kl_weight (float): Weighting of the KL divergence term in VI. The goal is to find a surrogate posterior `q(z)` that maximizes a version of the ELBO with the `KL(surrogate posterior || prior)` term scaled by `kl_weight` E_z~q [log p(x|z)] - kl_weight * KL(q || p) Reference > Weight Uncertainty in Neural Network > Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra. > Proceedings of the 32nd International Conference on Machine Learning. > PMLR 37:1613-1622, 2015. > <https://proceedings.mlr.press/v37/blundell15> batch_size (None | int): If specified, the log probability in each step of variational inference is computed on a batch of this size. Default is `None`, meaning full-batch. Returns: Instance of self. \"\"\" train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) _ , self . losses_ , self . params_ = inference . fit_vi ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , ensemble_size = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , sample_size_posterior = sample_size_posterior , sample_size_divergence = sample_size_divergence , kl_weight = kl_weight , batch_size = batch_size , ) return self","title":"BayesianNeuralFieldVI"},{"location":"api/BayesianNeuralFieldVI/#bayesianneuralfieldvi","text":"Bases: BayesianNeuralFieldEstimator Fits models using stochastic ensembles of surrogate posteriors from VI. Implementation of BayesianNeuralFieldEstimator using variational inference (VI). Source code in bayesnf/spatiotemporal.py 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 class BayesianNeuralFieldVI ( BayesianNeuralFieldEstimator ): \"\"\"Fits models using stochastic ensembles of surrogate posteriors from VI. Implementation of [BayesianNeuralFieldEstimator](BayesianNeuralFieldEstimator.md) using variational inference (VI). \"\"\" _ensemble_dims = 3 _scale_epochs_by_batch_size = True def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.01 , num_epochs = 1_000 , sample_size_posterior = 30 , sample_size_divergence = 5 , kl_weight = 0.1 , batch_size = None , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic variational inference ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles (i.e., surrogate posteriors) in the ensemble, **per device**. The available devices can be found via `jax.devices()`. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. sample_size_posterior (int): Number of samples of \"posterior\" model parameters draw from each surrogate posterior when making predictions. sample_size_divergence (int): number of Monte Carlo samples to use in estimating the variational divergence. Larger values may stabilize the optimization, but at higher cost per step in time and memory. See [`tfp.vi.fit_surrogate_posterior_stateless`]( https://www.tensorflow.org/probability/api_docs/python/tfp/vi/fit_surrogate_posterior_stateless) for further details. kl_weight (float): Weighting of the KL divergence term in VI. The goal is to find a surrogate posterior `q(z)` that maximizes a version of the ELBO with the `KL(surrogate posterior || prior)` term scaled by `kl_weight` E_z~q [log p(x|z)] - kl_weight * KL(q || p) Reference > Weight Uncertainty in Neural Network > Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra. > Proceedings of the 32nd International Conference on Machine Learning. > PMLR 37:1613-1622, 2015. > <https://proceedings.mlr.press/v37/blundell15> batch_size (None | int): If specified, the log probability in each step of variational inference is computed on a batch of this size. Default is `None`, meaning full-batch. Returns: Instance of self. \"\"\" train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) _ , self . losses_ , self . params_ = inference . fit_vi ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , ensemble_size = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , sample_size_posterior = sample_size_posterior , sample_size_divergence = sample_size_divergence , kl_weight = kl_weight , batch_size = batch_size , ) return self","title":"BayesianNeuralFieldVI"},{"location":"api/BayesianNeuralFieldVI/#bayesnf.spatiotemporal.BayesianNeuralFieldVI.fit","text":"fit ( table , seed , ensemble_size = 16 , learning_rate = 0.01 , num_epochs = 1000 , sample_size_posterior = 30 , sample_size_divergence = 5 , kl_weight = 0.1 , batch_size = None ) Run inference using stochastic variational inference ensembles. PARAMETER DESCRIPTION table See documentation of table in the base class. TYPE: DataFrame seed The jax random key. TYPE: PRNGKey ensemble_size Number of particles (i.e., surrogate posteriors) in the ensemble, per device . The available devices can be found via jax.devices() . TYPE: int DEFAULT: 16 learning_rate Learning rate for SGD. TYPE: float DEFAULT: 0.01 num_epochs Number of full epochs through the training data. TYPE: int DEFAULT: 1000 sample_size_posterior Number of samples of \"posterior\" model parameters draw from each surrogate posterior when making predictions. TYPE: int DEFAULT: 30 sample_size_divergence number of Monte Carlo samples to use in estimating the variational divergence. Larger values may stabilize the optimization, but at higher cost per step in time and memory. See tfp.vi.fit_surrogate_posterior_stateless for further details. TYPE: int DEFAULT: 5 kl_weight Weighting of the KL divergence term in VI. The goal is to find a surrogate posterior q(z) that maximizes a version of the ELBO with the KL(surrogate posterior || prior) term scaled by kl_weight E_z~q [log p(x|z)] - kl_weight * KL(q || p) Reference Weight Uncertainty in Neural Network Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra. Proceedings of the 32nd International Conference on Machine Learning. PMLR 37:1613-1622, 2015. https://proceedings.mlr.press/v37/blundell15 TYPE: float DEFAULT: 0.1 batch_size If specified, the log probability in each step of variational inference is computed on a batch of this size. Default is None , meaning full-batch. TYPE: None | int DEFAULT: None RETURNS DESCRIPTION BayesianNeuralFieldEstimator Instance of self. Source code in bayesnf/spatiotemporal.py 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 def fit ( self , table , seed , ensemble_size = 16 , learning_rate = 0.01 , num_epochs = 1_000 , sample_size_posterior = 30 , sample_size_divergence = 5 , kl_weight = 0.1 , batch_size = None , ) -> BayesianNeuralFieldEstimator : \"\"\"Run inference using stochastic variational inference ensembles. Args: table (pandas.DataFrame): See documentation of [`table`][bayesnf.spatiotemporal.BayesianNeuralFieldEstimator.fit] in the base class. seed (jax.random.PRNGKey): The jax random key. ensemble_size (int): Number of particles (i.e., surrogate posteriors) in the ensemble, **per device**. The available devices can be found via `jax.devices()`. learning_rate (float): Learning rate for SGD. num_epochs (int): Number of full epochs through the training data. sample_size_posterior (int): Number of samples of \"posterior\" model parameters draw from each surrogate posterior when making predictions. sample_size_divergence (int): number of Monte Carlo samples to use in estimating the variational divergence. Larger values may stabilize the optimization, but at higher cost per step in time and memory. See [`tfp.vi.fit_surrogate_posterior_stateless`]( https://www.tensorflow.org/probability/api_docs/python/tfp/vi/fit_surrogate_posterior_stateless) for further details. kl_weight (float): Weighting of the KL divergence term in VI. The goal is to find a surrogate posterior `q(z)` that maximizes a version of the ELBO with the `KL(surrogate posterior || prior)` term scaled by `kl_weight` E_z~q [log p(x|z)] - kl_weight * KL(q || p) Reference > Weight Uncertainty in Neural Network > Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra. > Proceedings of the 32nd International Conference on Machine Learning. > PMLR 37:1613-1622, 2015. > <https://proceedings.mlr.press/v37/blundell15> batch_size (None | int): If specified, the log probability in each step of variational inference is computed on a batch of this size. Default is `None`, meaning full-batch. Returns: Instance of self. \"\"\" train_data = self . data_handler . get_train ( table ) train_target = self . data_handler . get_target ( table ) if batch_size is None : batch_size = train_data . shape [ 0 ] if self . _scale_epochs_by_batch_size : num_epochs = num_epochs * ( train_data . shape [ 0 ] // batch_size ) model_args = self . _model_args (( batch_size , train_data . shape [ - 1 ])) _ , self . losses_ , self . params_ = inference . fit_vi ( train_data , train_target , seed = seed , observation_model = self . observation_model , model_args = model_args , ensemble_size = ensemble_size , learning_rate = learning_rate , num_epochs = num_epochs , sample_size_posterior = sample_size_posterior , sample_size_divergence = sample_size_divergence , kl_weight = kl_weight , batch_size = batch_size , ) return self","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;fit"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/","text":"Hungarian Chickenpox Tutorial \u00a4 # Download latest version of bayesnf. ! pip install bayesnf # Install Python libraries for plotting. ! pip - q install cartopy ! pip - q install contextily ! pip - q install geopandas # Load the shape file for geopandas visualization. ! wget - q https : // www . geoboundaries . org / data / 1_3_3 / zip / shapefile / HUN / HUN_ADM1 . shp . zip ! unzip - oq HUN_ADM1 . shp . zip import warnings warnings . simplefilter ( 'ignore' ) import contextily as ctx import geopandas as gpd import jax import matplotlib.pyplot as plt import numpy as np import pandas as pd from cartopy import crs as ccrs from shapely.geometry import Point from mpl_toolkits.axes_grid1 import make_axes_locatable Loading and Plotting Data \u00a4 We analyze the Hungarian Chickenpox Cases spatiotemporal dataset from the UCI machine learning repository https://archive.ics.uci.edu/dataset/580/hungarian+chickenpox+cases . The data contains 20 county-level time series of weekly chickenpox cases in Hungary between 2005 and 2015. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / chickenpox .5 . train . csv df_train = pd . read_csv ( 'chickenpox.5.train.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) BayesNF excepts dataframe to be in \"long\" format. That is, each row shows a single observation ( chickenpox column) at a given point in time ( datetime column) and in space ( latitude and longitude columns, which show the centroid of the county). The location column is metadata that provides a human-readable name for the county at which measurement was recorded. df_train . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } location datetime latitude longitude chickenpox 1044 BACS 2005-01-03 46.568416 19.379846 30 1045 BACS 2005-01-10 46.568416 19.379846 30 1046 BACS 2005-01-17 46.568416 19.379846 31 1047 BACS 2005-01-24 46.568416 19.379846 43 1048 BACS 2005-01-31 46.568416 19.379846 53 1049 BACS 2005-02-07 46.568416 19.379846 77 1050 BACS 2005-02-14 46.568416 19.379846 54 1051 BACS 2005-02-21 46.568416 19.379846 64 1052 BACS 2005-02-28 46.568416 19.379846 57 1053 BACS 2005-03-07 46.568416 19.379846 129 1054 BACS 2005-03-14 46.568416 19.379846 81 1055 BACS 2005-03-21 46.568416 19.379846 51 1056 BACS 2005-03-28 46.568416 19.379846 98 1057 BACS 2005-04-04 46.568416 19.379846 59 1058 BACS 2005-04-11 46.568416 19.379846 84 1059 BACS 2005-04-18 46.568416 19.379846 62 1060 BACS 2005-04-25 46.568416 19.379846 120 1061 BACS 2005-05-02 46.568416 19.379846 81 1062 BACS 2005-05-09 46.568416 19.379846 103 1063 BACS 2005-05-16 46.568416 19.379846 86 We can use the geopandas library to plot snapshots the data over the spatial field at different points in time. # Create a dataframe for plotting using geopandas. hungary = gpd . read_file ( 'HUN_ADM1.shp' ) df_plot = df_train . copy () df_plot [ 'centroid' ] = df_plot [[ 'longitude' , 'latitude' ]] . apply ( Point , axis = 1 ) centroid_to_polygon = { c : next ( g for g in hungary . geometry . values if g . contains ( c )) for c in set ( df_plot [ 'centroid' ]) } df_plot [ 'boundary' ] = df_plot [ 'centroid' ] . replace ( centroid_to_polygon ) # Helper function to plot a single map. def plot_map ( date , ax ): # Plot basemap. hungary . plot ( color = 'none' , edgecolor = 'black' , linewidth = 1 , ax = ax ) ctx . add_basemap ( ax , crs = hungary . crs . to_string (), attribution = '' , zorder =- 1 ) # Make legend axes. divider = make_axes_locatable ( ax ) cax = divider . append_axes ( 'right' , size = '5%' , pad = '2%' , axes_class = plt . matplotlib . axes . Axes ) # Set date # Plot stations. df_plot_geo = gpd . GeoDataFrame ( df_plot , geometry = 'boundary' ) df_plot_geo_t0 = df_plot_geo [ df_plot_geo . datetime == date ] df_plot_geo_t0 . plot ( column = 'chickenpox' , alpha = .5 , vmin = 0 , vmax = 200 , edgecolor = 'k' , linewidth = 1 , legend = True , cmap = 'jet' , cax = cax , ax = ax ) gl = ax . gridlines ( draw_labels = True , alpha = 0 ) gl . top_labels = False gl . right_labels = False ax . set_title ( date ) fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , subplot_kw = { 'projection' : ccrs . PlateCarree ()}, figsize = ( 12.5 , 12.5 ), tight_layout = True ) dates = [ '2005-01-03' , '2005-02-28' , '2005-03-07' , '2005-05-16' ] for ax , date in zip ( axes . flat , dates ): plot_map ( date , ax ) We can also plot the observed time series at each of the 20 spatial locations. The data clarify several patterns: - There is a low-frequency (annual) seasonal effect. - There is possibly a higher-frequency (monthly) seasonal effect. - Locations closer in space demonstrate the strongest cross-covariance. - As the data is not normalized per-capita, the amplitude of the time series (on the y-axes) generally vary across counties. locations = df_train . location . unique () fig , axes = plt . subplots ( ncols = 4 , nrows = 5 , tight_layout = True , figsize = ( 25 , 20 )) for ax , location in zip ( axes . flat , locations ): df_location = df_train [ df_train . location == location ] latitude , longitude = df_location . iloc [ 0 ][[ 'latitude' , 'longitude' ]] ax . plot ( df_location . datetime , df_location . chickenpox , marker = '.' , color = 'k' , linewidth = 1 ) ax . set_title ( f 'County: { location } ( { longitude : .2f } , { latitude : .2f } )' ) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'Chickenpox Cases' ) Spatiotemporal Prediction with BayesNF \u00a4 The next step is to construct a BayesNF model . Since this dataset consists of areal (or lattice) measurements, we represent the spatial locations using the centroid (in (latitude, longitude) coordinates) of each county. Building an Estimator \u00a4 BayesNF provides three different estimation methods: BayesianNeuralFieldMAP estimator, which performs inference using stochastic ensembles of maximum-a-posteriori estimates. BayesianNeuralFieldVI which uses ensemble of posterior surrogates learned using variational Bayesian inference . BayesianNeuralFieldMLE , which uses an ensemble of maximum likelihood estimates . All of these estimators satisfy the same API of the abstract BayesianNeuralFieldEstimator class. We will use the MAP version in this tutorial. from bayesnf.spatiotemporal import BayesianNeuralFieldMAP model = BayesianNeuralFieldMAP ( width = 256 , depth = 2 , freq = 'W' , seasonality_periods = [ 'M' , 'Y' ], # equivalent to [365.25/12, 365.25] num_seasonal_harmonics = [ 2 , 10 ], # two harmonics for M; one harmonic for Y feature_cols = [ 'datetime' , 'latitude' , 'longitude' ], # time, spatial 1, ..., spatial n target_col = 'chickenpox' , observation_model = 'NORMAL' , timetype = 'index' , standardize = [ 'latitude' , 'longitude' ], interactions = [( 0 , 1 ), ( 0 , 2 ), ( 1 , 2 )], ) Fitting the Estimator \u00a4 All three estimators provide a .fit method, with slightly different signatures. The configuration below trains an ensemble comprised of 64 particles for 5000 epochs. These commands require ~120 seconds on a TPU v3-8; the ensemble_size and num_epochs values should be adjusted depending on the available resources. # Train MAP ensemble model = model . fit ( df_train , seed = jax . random . PRNGKey ( 0 ), ensemble_size = 64 , num_epochs = 5000 , ) Plotting Training Loss \u00a4 Plotting training loss gives us a sense of convergence of the learning dynamics and agreement among differnet members of the ensemble. # Inspect the training loss for each particle. import matplotlib.pyplot as plt losses = np . row_stack ( model . losses_ ) fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . plot ( losses . T ) ax . plot ( np . mean ( losses , axis = 0 ), color = 'k' , linewidth = 3 ) ax . set_xlabel ( 'Epoch' ) ax . set_ylabel ( 'Negative Joint Probability' ) ax . set_yscale ( 'log' , base = 10 ) Making Predictions \u00a4 The predict method takes in a test data frame, with the same format as the training data frame, except without the target column; quantiles, which are a list of numbers between 0 and 1. It returns mean predictions yhat and the requested quantiles yhat_quantiles . The yhat estimates are returned separately for each member of the ensemble whereas the yhat_quantiles estimates are computed across the entire ensemble. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / chickenpox .5 . test . csv df_test = pd . read_csv ( 'chickenpox.5.test.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) yhat , yhat_quantiles = model . predict ( df_test , quantiles = ( 0.025 , 0.5 , 0.975 )) It is helpful to show a scatter plot of the true vs predicted values on the test data. We will plot the median predictions yhat_quantiles[1] versus the true chickenpox value. fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . scatter ( df_test . chickenpox , yhat_quantiles [ 1 ], marker = '.' , color = 'k' ) ax . plot ([ 0 , 250 ], [ 0 , 250 ], color = 'red' ) ax . set_xlabel ( 'True Value' ) ax . set_ylabel ( 'Predicted Value' ) Text(0, 0.5, 'Predicted Value') We can also show the forecats on the held-out data for each of the four counties in the test set. locations = df_test . location . unique () fig , axes = plt . subplots ( nrows = 2 , ncols = len ( locations ) // 2 , tight_layout = True , figsize = ( 16 , 8 )) for ax , location in zip ( axes . flat , locations ): y_train = df_train [ df_train . location == location ] y_test = df_test [ df_test . location == location ] ax . scatter ( y_train . datetime [ - 100 :], y_train . chickenpox [ - 100 :], marker = 'o' , color = 'k' , label = 'Observations' ) ax . scatter ( y_test . datetime , y_test . chickenpox , marker = 'o' , edgecolor = 'k' , facecolor = 'w' , label = 'Test Data' ) mask = df_test . location . to_numpy () == location ax . plot ( y_test . datetime , yhat_quantiles [ 1 ][ mask ], color = 'red' , label = 'Meidan Prediction' ) ax . fill_between ( y_test . datetime , yhat_quantiles [ 0 ][ mask ], yhat_quantiles [ 2 ][ mask ], alpha = 0.5 , label = '95% Prediction Interval' ) ax . set_title ( 'Test Location: %s ' % ( location ,)) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'Flu Cases' ) axes . flat [ 0 ] . legend ( loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f0797df1690>","title":"Hungarian Chickenpox"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#hungarian-chickenpox-tutorial","text":"# Download latest version of bayesnf. ! pip install bayesnf # Install Python libraries for plotting. ! pip - q install cartopy ! pip - q install contextily ! pip - q install geopandas # Load the shape file for geopandas visualization. ! wget - q https : // www . geoboundaries . org / data / 1_3_3 / zip / shapefile / HUN / HUN_ADM1 . shp . zip ! unzip - oq HUN_ADM1 . shp . zip import warnings warnings . simplefilter ( 'ignore' ) import contextily as ctx import geopandas as gpd import jax import matplotlib.pyplot as plt import numpy as np import pandas as pd from cartopy import crs as ccrs from shapely.geometry import Point from mpl_toolkits.axes_grid1 import make_axes_locatable","title":"Hungarian Chickenpox Tutorial"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#loading-and-plotting-data","text":"We analyze the Hungarian Chickenpox Cases spatiotemporal dataset from the UCI machine learning repository https://archive.ics.uci.edu/dataset/580/hungarian+chickenpox+cases . The data contains 20 county-level time series of weekly chickenpox cases in Hungary between 2005 and 2015. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / chickenpox .5 . train . csv df_train = pd . read_csv ( 'chickenpox.5.train.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) BayesNF excepts dataframe to be in \"long\" format. That is, each row shows a single observation ( chickenpox column) at a given point in time ( datetime column) and in space ( latitude and longitude columns, which show the centroid of the county). The location column is metadata that provides a human-readable name for the county at which measurement was recorded. df_train . head ( 20 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } location datetime latitude longitude chickenpox 1044 BACS 2005-01-03 46.568416 19.379846 30 1045 BACS 2005-01-10 46.568416 19.379846 30 1046 BACS 2005-01-17 46.568416 19.379846 31 1047 BACS 2005-01-24 46.568416 19.379846 43 1048 BACS 2005-01-31 46.568416 19.379846 53 1049 BACS 2005-02-07 46.568416 19.379846 77 1050 BACS 2005-02-14 46.568416 19.379846 54 1051 BACS 2005-02-21 46.568416 19.379846 64 1052 BACS 2005-02-28 46.568416 19.379846 57 1053 BACS 2005-03-07 46.568416 19.379846 129 1054 BACS 2005-03-14 46.568416 19.379846 81 1055 BACS 2005-03-21 46.568416 19.379846 51 1056 BACS 2005-03-28 46.568416 19.379846 98 1057 BACS 2005-04-04 46.568416 19.379846 59 1058 BACS 2005-04-11 46.568416 19.379846 84 1059 BACS 2005-04-18 46.568416 19.379846 62 1060 BACS 2005-04-25 46.568416 19.379846 120 1061 BACS 2005-05-02 46.568416 19.379846 81 1062 BACS 2005-05-09 46.568416 19.379846 103 1063 BACS 2005-05-16 46.568416 19.379846 86 We can use the geopandas library to plot snapshots the data over the spatial field at different points in time. # Create a dataframe for plotting using geopandas. hungary = gpd . read_file ( 'HUN_ADM1.shp' ) df_plot = df_train . copy () df_plot [ 'centroid' ] = df_plot [[ 'longitude' , 'latitude' ]] . apply ( Point , axis = 1 ) centroid_to_polygon = { c : next ( g for g in hungary . geometry . values if g . contains ( c )) for c in set ( df_plot [ 'centroid' ]) } df_plot [ 'boundary' ] = df_plot [ 'centroid' ] . replace ( centroid_to_polygon ) # Helper function to plot a single map. def plot_map ( date , ax ): # Plot basemap. hungary . plot ( color = 'none' , edgecolor = 'black' , linewidth = 1 , ax = ax ) ctx . add_basemap ( ax , crs = hungary . crs . to_string (), attribution = '' , zorder =- 1 ) # Make legend axes. divider = make_axes_locatable ( ax ) cax = divider . append_axes ( 'right' , size = '5%' , pad = '2%' , axes_class = plt . matplotlib . axes . Axes ) # Set date # Plot stations. df_plot_geo = gpd . GeoDataFrame ( df_plot , geometry = 'boundary' ) df_plot_geo_t0 = df_plot_geo [ df_plot_geo . datetime == date ] df_plot_geo_t0 . plot ( column = 'chickenpox' , alpha = .5 , vmin = 0 , vmax = 200 , edgecolor = 'k' , linewidth = 1 , legend = True , cmap = 'jet' , cax = cax , ax = ax ) gl = ax . gridlines ( draw_labels = True , alpha = 0 ) gl . top_labels = False gl . right_labels = False ax . set_title ( date ) fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , subplot_kw = { 'projection' : ccrs . PlateCarree ()}, figsize = ( 12.5 , 12.5 ), tight_layout = True ) dates = [ '2005-01-03' , '2005-02-28' , '2005-03-07' , '2005-05-16' ] for ax , date in zip ( axes . flat , dates ): plot_map ( date , ax ) We can also plot the observed time series at each of the 20 spatial locations. The data clarify several patterns: - There is a low-frequency (annual) seasonal effect. - There is possibly a higher-frequency (monthly) seasonal effect. - Locations closer in space demonstrate the strongest cross-covariance. - As the data is not normalized per-capita, the amplitude of the time series (on the y-axes) generally vary across counties. locations = df_train . location . unique () fig , axes = plt . subplots ( ncols = 4 , nrows = 5 , tight_layout = True , figsize = ( 25 , 20 )) for ax , location in zip ( axes . flat , locations ): df_location = df_train [ df_train . location == location ] latitude , longitude = df_location . iloc [ 0 ][[ 'latitude' , 'longitude' ]] ax . plot ( df_location . datetime , df_location . chickenpox , marker = '.' , color = 'k' , linewidth = 1 ) ax . set_title ( f 'County: { location } ( { longitude : .2f } , { latitude : .2f } )' ) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'Chickenpox Cases' )","title":"Loading and Plotting Data"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#spatiotemporal-prediction-with-bayesnf","text":"The next step is to construct a BayesNF model . Since this dataset consists of areal (or lattice) measurements, we represent the spatial locations using the centroid (in (latitude, longitude) coordinates) of each county.","title":"Spatiotemporal Prediction with BayesNF"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#building-an-estimator","text":"BayesNF provides three different estimation methods: BayesianNeuralFieldMAP estimator, which performs inference using stochastic ensembles of maximum-a-posteriori estimates. BayesianNeuralFieldVI which uses ensemble of posterior surrogates learned using variational Bayesian inference . BayesianNeuralFieldMLE , which uses an ensemble of maximum likelihood estimates . All of these estimators satisfy the same API of the abstract BayesianNeuralFieldEstimator class. We will use the MAP version in this tutorial. from bayesnf.spatiotemporal import BayesianNeuralFieldMAP model = BayesianNeuralFieldMAP ( width = 256 , depth = 2 , freq = 'W' , seasonality_periods = [ 'M' , 'Y' ], # equivalent to [365.25/12, 365.25] num_seasonal_harmonics = [ 2 , 10 ], # two harmonics for M; one harmonic for Y feature_cols = [ 'datetime' , 'latitude' , 'longitude' ], # time, spatial 1, ..., spatial n target_col = 'chickenpox' , observation_model = 'NORMAL' , timetype = 'index' , standardize = [ 'latitude' , 'longitude' ], interactions = [( 0 , 1 ), ( 0 , 2 ), ( 1 , 2 )], )","title":"Building an Estimator"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#fitting-the-estimator","text":"All three estimators provide a .fit method, with slightly different signatures. The configuration below trains an ensemble comprised of 64 particles for 5000 epochs. These commands require ~120 seconds on a TPU v3-8; the ensemble_size and num_epochs values should be adjusted depending on the available resources. # Train MAP ensemble model = model . fit ( df_train , seed = jax . random . PRNGKey ( 0 ), ensemble_size = 64 , num_epochs = 5000 , )","title":"Fitting the Estimator"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#plotting-training-loss","text":"Plotting training loss gives us a sense of convergence of the learning dynamics and agreement among differnet members of the ensemble. # Inspect the training loss for each particle. import matplotlib.pyplot as plt losses = np . row_stack ( model . losses_ ) fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . plot ( losses . T ) ax . plot ( np . mean ( losses , axis = 0 ), color = 'k' , linewidth = 3 ) ax . set_xlabel ( 'Epoch' ) ax . set_ylabel ( 'Negative Joint Probability' ) ax . set_yscale ( 'log' , base = 10 )","title":"Plotting Training Loss"},{"location":"tutorials/BayesNF_Tutorial_on_Hungarian_Chickenpox/#making-predictions","text":"The predict method takes in a test data frame, with the same format as the training data frame, except without the target column; quantiles, which are a list of numbers between 0 and 1. It returns mean predictions yhat and the requested quantiles yhat_quantiles . The yhat estimates are returned separately for each member of the ensemble whereas the yhat_quantiles estimates are computed across the entire ensemble. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / chickenpox .5 . test . csv df_test = pd . read_csv ( 'chickenpox.5.test.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) yhat , yhat_quantiles = model . predict ( df_test , quantiles = ( 0.025 , 0.5 , 0.975 )) It is helpful to show a scatter plot of the true vs predicted values on the test data. We will plot the median predictions yhat_quantiles[1] versus the true chickenpox value. fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . scatter ( df_test . chickenpox , yhat_quantiles [ 1 ], marker = '.' , color = 'k' ) ax . plot ([ 0 , 250 ], [ 0 , 250 ], color = 'red' ) ax . set_xlabel ( 'True Value' ) ax . set_ylabel ( 'Predicted Value' ) Text(0, 0.5, 'Predicted Value') We can also show the forecats on the held-out data for each of the four counties in the test set. locations = df_test . location . unique () fig , axes = plt . subplots ( nrows = 2 , ncols = len ( locations ) // 2 , tight_layout = True , figsize = ( 16 , 8 )) for ax , location in zip ( axes . flat , locations ): y_train = df_train [ df_train . location == location ] y_test = df_test [ df_test . location == location ] ax . scatter ( y_train . datetime [ - 100 :], y_train . chickenpox [ - 100 :], marker = 'o' , color = 'k' , label = 'Observations' ) ax . scatter ( y_test . datetime , y_test . chickenpox , marker = 'o' , edgecolor = 'k' , facecolor = 'w' , label = 'Test Data' ) mask = df_test . location . to_numpy () == location ax . plot ( y_test . datetime , yhat_quantiles [ 1 ][ mask ], color = 'red' , label = 'Meidan Prediction' ) ax . fill_between ( y_test . datetime , yhat_quantiles [ 0 ][ mask ], yhat_quantiles [ 2 ][ mask ], alpha = 0.5 , label = '95% Prediction Interval' ) ax . set_title ( 'Test Location: %s ' % ( location ,)) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'Flu Cases' ) axes . flat [ 0 ] . legend ( loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f0797df1690>","title":"Making Predictions"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/","text":"London Air Quality Tutorial \u00a4 # Download latest version of bayesnf. ! pip install - q bayesnf # Install Python libraries for plotting. ! pip - q install cartopy ! pip - q install contextily ! pip - q install geopandas import warnings warnings . simplefilter ( 'ignore' ) import contextily as ctx import geopandas as gpd import jax import matplotlib.pyplot as plt import numpy as np import pandas as pd from cartopy import crs as ccrs from shapely.geometry import Point from mpl_toolkits.axes_grid1 import make_axes_locatable Loading and Plotting Data \u00a4 We analyze data from the London Air Quality Network https://www.londonair.org.uk/ . The data contains air pollution levels measured by sensors across London, measured every hour. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / air_quality .5 . train . csv df_train = pd . read_csv ( 'air_quality.5.train.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) BayesNF excepts dataframe to be in \"long\" format. That is, each row shows a single observation of pm10 , which measures the atmospheric particulate matter at a given point in time ( datetime column) and in space ( latitude and longitude columns, which show the location of the sensor). df_train . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } location datetime latitude longitude pm10 0 0 2018-12-31 19:00:00 51.35866 -0.149724 22.433324 1 0 2018-12-31 20:00:00 51.35866 -0.149724 19.833325 2 0 2018-12-31 21:00:00 51.35866 -0.149724 11.833329 3 0 2018-12-31 22:00:00 51.35866 -0.149724 10.833329 4 0 2018-12-31 23:00:00 51.35866 -0.149724 13.833328 5 0 2019-01-01 00:00:00 51.35866 -0.149724 14.433328 6 0 2019-01-01 01:00:00 51.35866 -0.149724 12.033329 7 0 2019-01-01 02:00:00 51.35866 -0.149724 13.233328 8 0 2019-01-01 03:00:00 51.35866 -0.149724 12.433328 9 0 2019-01-01 04:00:00 51.35866 -0.149724 13.633328 We can use the geopandas library to plot snapshots of the data over the spatial field at different points in time. ! wget - q https : // data . london . gov . uk / download / statistical - gis - boundary - files - london / 9 ba8c833 - 6370 - 4 b11 - abdc - 314 aa020d5e0 / statistical - gis - boundaries - london . zip ! unzip - oq statistical - gis - boundaries - london . zip # Create a dataframe for plotting using geopandas. london = gpd . read_file ( \"statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\" ) london = london . to_crs ( epsg = 4326 ) df_plot = df_train . copy () stations = [ Point ( c ) for c in zip ( df_plot [ 'longitude' ], df_plot [ 'latitude' ])] # Helper function to plot a single map. def plot_map ( date , ax ): # Plot basemap. london . plot ( color = 'none' , edgecolor = 'black' , linewidth = 1 , aspect = 1 , ax = ax ) ctx . add_basemap ( ax , crs = london . crs . to_string (), attribution = '' , zorder =- 1 ) # Make legend axes. divider = make_axes_locatable ( ax ) cax = divider . append_axes ( 'right' , size = '5%' , pad = '2%' , axes_class = plt . matplotlib . axes . Axes ) # Set date # Plot stations. df_plot_geo = gpd . GeoDataFrame ( df_plot , crs = london . crs , geometry = stations ) df_plot_geo_t0 = df_plot_geo [ df_plot_geo . datetime == date ] df_plot_geo_t0 . plot ( column = 'pm10' , markersize = 25 * df_plot_geo_t0 [ 'pm10' ] . values , marker = 'o' , alpha = .25 , edgecolor = 'k' , linewidth = 1 , legend = True , legend_kwds = { 'pad' : 5 , 'orientation' : 'vertical' }, cmap = plt . colormaps [ 'jet' ], cax = cax , ax = ax , aspect = 1 , ) ax . scatter ( df_plot_geo_t0 . longitude , df_plot_geo_t0 . latitude , marker = 's' , s = 4 , color = 'k' ) gl = ax . gridlines ( draw_labels = True , alpha = 0 ) gl . top_labels = False gl . right_labels = False ax . set_title ( date ) fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , subplot_kw = { 'projection' : ccrs . PlateCarree ()}, figsize = ( 12.5 , 12.5 ), tight_layout = True ) dates = [ '2019-01-01 08:00:00' , '2019-01-02 08:00:00' , '2019-01-03 08:00:00' , '2019-01-04 12:00:00' ] for ax , date in zip ( axes . flat , dates ): plot_map ( date , ax ) We can also plot the observed time series at 20 representative locations. locations = df_train . location . unique () fig , axes = plt . subplots ( ncols = 4 , nrows = 5 , tight_layout = True , figsize = ( 30 , 20 )) for ax , location in zip ( axes . flat , locations ): df_location = df_train [ df_train . location == location ] latitude , longitude = df_location . iloc [ 0 ][[ 'latitude' , 'longitude' ]] ax . plot ( df_location . datetime , df_location . pm10 , marker = '.' , color = 'k' , linewidth = 1 ) ax . set_title ( f 'Sensor Location ( { longitude : .2f } , { latitude : .2f } )' ) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'PM10 Measurement' ) Spatiotemporal Prediction with BayesNF \u00a4 The next step is to construct a BayesNF model . The spatial locations are represented using the (latitude, longitude) coordinates of each sensor. Building an Estimator \u00a4 BayesNF provides three different estimation methods: BayesianNeuralFieldMAP estimator, which performs inference using stochastic ensembles of maximum-a-posteriori estimates. BayesianNeuralFieldVI which uses ensemble of posterior surrogates learned using variational Bayesian inference . BayesianNeuralFieldMLE , which uses an ensemble of maximum likelihood estimates . All of these estimators satisfy the same API of the abstract BayesianNeuralFieldEstimator class. We will use the MAP version in this tutorial. from bayesnf.spatiotemporal import BayesianNeuralFieldMAP model = BayesianNeuralFieldMAP ( width = 512 , depth = 2 , freq = 'H' , seasonality_periods = [ 'D' , 'W' ], # Daily and weekly seasonality, same as [24, 24*7] num_seasonal_harmonics = [ 4 , 4 ], # Four harmonics for each seasonal factor. feature_cols = [ 'datetime' , 'latitude' , 'longitude' ], # time, spatial 1, ..., spatial n target_col = 'pm10' , observation_model = 'NORMAL' , timetype = 'index' , standardize = [ 'latitude' , 'longitude' ], ) Fitting the Estimator \u00a4 All three estimators provide a .fit method, with slightly different signatures. The configuration below trains an ensemble comprised of 64 particles for 5000 epochs. These commands require around 3 minutes on a TPU v3-8; the ensemble_size and num_epochs values should be adjusted depending on the available resources. # Train MAP ensemble model = model . fit ( df_train , seed = jax . random . PRNGKey ( 0 ), ensemble_size = 8 , num_epochs = 5000 , ) Plotting Training Loss \u00a4 Plotting training loss gives us a sense of convergence of the learning dynamics and agreement among differnet members of the ensemble. # Inspect the training loss for each particle. import matplotlib.pyplot as plt losses = np . row_stack ( model . losses_ ) fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . plot ( losses . T ) ax . plot ( np . mean ( losses , axis = 0 ), color = 'k' , linewidth = 3 ) ax . set_xlabel ( 'Epoch' ) ax . set_ylabel ( 'Negative Joint Probability' ) ax . set_yscale ( 'log' , base = 10 ) Making Predictions \u00a4 The predict method takes in a test data frame, with the same format as the training data frame, except without the target column; quantiles, which are a list of numbers between 0 and 1. It returns mean predictions yhat and the requested quantiles yhat_quantiles . The yhat estimates are returned separately for each member of the ensemble whereas the yhat_quantiles estimates are computed across the entire ensemble. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / air_quality .5 . test . csv df_test = pd . read_csv ( 'air_quality.5.test.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) yhat , yhat_quantiles = model . predict ( df_test , quantiles = ( 0.025 , 0.5 , 0.975 )) It is helpful to show a scatter plot of the true vs predicted values on the test data. We will plot the median predictions yhat_quantiles[1] versus the true chickenpox value. fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . scatter ( df_test . pm10 , yhat_quantiles [ 1 ], marker = '.' , color = 'k' ) ax . plot ([ 0 , 250 ], [ 0 , 250 ], color = 'red' ) ax . set_xlabel ( 'True Value' ) ax . set_ylabel ( 'Predicted Value' ) Text(0, 0.5, 'Predicted Value') We can also show the forecats on the held-out data for each of the four counties in the test set. locations = df_test . location . unique () fig , axes = plt . subplots ( ncols = 4 , nrows = len ( locations ) // 4 , tight_layout = True , figsize = ( 16 , 10 )) for ax , location in zip ( axes . flat , locations ): y_train = df_train [ df_train . location == location ] y_test = df_test [ df_test . location == location ] ax . scatter ( y_train . datetime [ - 100 :], y_train . pm10 [ - 100 :], marker = 'o' , color = 'k' , label = 'Observations' ) ax . scatter ( y_test . datetime , y_test . pm10 , marker = 'o' , edgecolor = 'k' , facecolor = 'w' , label = 'Test Data' ) mask = df_test . location . to_numpy () == location ax . plot ( y_test . datetime , yhat_quantiles [ 1 ][ mask ], color = 'red' , label = 'Meidan Prediction' ) ax . fill_between ( y_test . datetime , yhat_quantiles [ 0 ][ mask ], yhat_quantiles [ 2 ][ mask ], alpha = 0.5 , label = '95% Prediction Interval' ) ax . set_title ( 'Test Location: %s ' % ( location ,)) ax . set_xticks ([]) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'PM10' ) axes . flat [ 0 ] . legend ( loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f01f0413be0>","title":"London Air Quality"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#london-air-quality-tutorial","text":"# Download latest version of bayesnf. ! pip install - q bayesnf # Install Python libraries for plotting. ! pip - q install cartopy ! pip - q install contextily ! pip - q install geopandas import warnings warnings . simplefilter ( 'ignore' ) import contextily as ctx import geopandas as gpd import jax import matplotlib.pyplot as plt import numpy as np import pandas as pd from cartopy import crs as ccrs from shapely.geometry import Point from mpl_toolkits.axes_grid1 import make_axes_locatable","title":"London Air Quality Tutorial"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#loading-and-plotting-data","text":"We analyze data from the London Air Quality Network https://www.londonair.org.uk/ . The data contains air pollution levels measured by sensors across London, measured every hour. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / air_quality .5 . train . csv df_train = pd . read_csv ( 'air_quality.5.train.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) BayesNF excepts dataframe to be in \"long\" format. That is, each row shows a single observation of pm10 , which measures the atmospheric particulate matter at a given point in time ( datetime column) and in space ( latitude and longitude columns, which show the location of the sensor). df_train . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } location datetime latitude longitude pm10 0 0 2018-12-31 19:00:00 51.35866 -0.149724 22.433324 1 0 2018-12-31 20:00:00 51.35866 -0.149724 19.833325 2 0 2018-12-31 21:00:00 51.35866 -0.149724 11.833329 3 0 2018-12-31 22:00:00 51.35866 -0.149724 10.833329 4 0 2018-12-31 23:00:00 51.35866 -0.149724 13.833328 5 0 2019-01-01 00:00:00 51.35866 -0.149724 14.433328 6 0 2019-01-01 01:00:00 51.35866 -0.149724 12.033329 7 0 2019-01-01 02:00:00 51.35866 -0.149724 13.233328 8 0 2019-01-01 03:00:00 51.35866 -0.149724 12.433328 9 0 2019-01-01 04:00:00 51.35866 -0.149724 13.633328 We can use the geopandas library to plot snapshots of the data over the spatial field at different points in time. ! wget - q https : // data . london . gov . uk / download / statistical - gis - boundary - files - london / 9 ba8c833 - 6370 - 4 b11 - abdc - 314 aa020d5e0 / statistical - gis - boundaries - london . zip ! unzip - oq statistical - gis - boundaries - london . zip # Create a dataframe for plotting using geopandas. london = gpd . read_file ( \"statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\" ) london = london . to_crs ( epsg = 4326 ) df_plot = df_train . copy () stations = [ Point ( c ) for c in zip ( df_plot [ 'longitude' ], df_plot [ 'latitude' ])] # Helper function to plot a single map. def plot_map ( date , ax ): # Plot basemap. london . plot ( color = 'none' , edgecolor = 'black' , linewidth = 1 , aspect = 1 , ax = ax ) ctx . add_basemap ( ax , crs = london . crs . to_string (), attribution = '' , zorder =- 1 ) # Make legend axes. divider = make_axes_locatable ( ax ) cax = divider . append_axes ( 'right' , size = '5%' , pad = '2%' , axes_class = plt . matplotlib . axes . Axes ) # Set date # Plot stations. df_plot_geo = gpd . GeoDataFrame ( df_plot , crs = london . crs , geometry = stations ) df_plot_geo_t0 = df_plot_geo [ df_plot_geo . datetime == date ] df_plot_geo_t0 . plot ( column = 'pm10' , markersize = 25 * df_plot_geo_t0 [ 'pm10' ] . values , marker = 'o' , alpha = .25 , edgecolor = 'k' , linewidth = 1 , legend = True , legend_kwds = { 'pad' : 5 , 'orientation' : 'vertical' }, cmap = plt . colormaps [ 'jet' ], cax = cax , ax = ax , aspect = 1 , ) ax . scatter ( df_plot_geo_t0 . longitude , df_plot_geo_t0 . latitude , marker = 's' , s = 4 , color = 'k' ) gl = ax . gridlines ( draw_labels = True , alpha = 0 ) gl . top_labels = False gl . right_labels = False ax . set_title ( date ) fig , axes = plt . subplots ( nrows = 2 , ncols = 2 , subplot_kw = { 'projection' : ccrs . PlateCarree ()}, figsize = ( 12.5 , 12.5 ), tight_layout = True ) dates = [ '2019-01-01 08:00:00' , '2019-01-02 08:00:00' , '2019-01-03 08:00:00' , '2019-01-04 12:00:00' ] for ax , date in zip ( axes . flat , dates ): plot_map ( date , ax ) We can also plot the observed time series at 20 representative locations. locations = df_train . location . unique () fig , axes = plt . subplots ( ncols = 4 , nrows = 5 , tight_layout = True , figsize = ( 30 , 20 )) for ax , location in zip ( axes . flat , locations ): df_location = df_train [ df_train . location == location ] latitude , longitude = df_location . iloc [ 0 ][[ 'latitude' , 'longitude' ]] ax . plot ( df_location . datetime , df_location . pm10 , marker = '.' , color = 'k' , linewidth = 1 ) ax . set_title ( f 'Sensor Location ( { longitude : .2f } , { latitude : .2f } )' ) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'PM10 Measurement' )","title":"Loading and Plotting Data"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#spatiotemporal-prediction-with-bayesnf","text":"The next step is to construct a BayesNF model . The spatial locations are represented using the (latitude, longitude) coordinates of each sensor.","title":"Spatiotemporal Prediction with BayesNF"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#building-an-estimator","text":"BayesNF provides three different estimation methods: BayesianNeuralFieldMAP estimator, which performs inference using stochastic ensembles of maximum-a-posteriori estimates. BayesianNeuralFieldVI which uses ensemble of posterior surrogates learned using variational Bayesian inference . BayesianNeuralFieldMLE , which uses an ensemble of maximum likelihood estimates . All of these estimators satisfy the same API of the abstract BayesianNeuralFieldEstimator class. We will use the MAP version in this tutorial. from bayesnf.spatiotemporal import BayesianNeuralFieldMAP model = BayesianNeuralFieldMAP ( width = 512 , depth = 2 , freq = 'H' , seasonality_periods = [ 'D' , 'W' ], # Daily and weekly seasonality, same as [24, 24*7] num_seasonal_harmonics = [ 4 , 4 ], # Four harmonics for each seasonal factor. feature_cols = [ 'datetime' , 'latitude' , 'longitude' ], # time, spatial 1, ..., spatial n target_col = 'pm10' , observation_model = 'NORMAL' , timetype = 'index' , standardize = [ 'latitude' , 'longitude' ], )","title":"Building an Estimator"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#fitting-the-estimator","text":"All three estimators provide a .fit method, with slightly different signatures. The configuration below trains an ensemble comprised of 64 particles for 5000 epochs. These commands require around 3 minutes on a TPU v3-8; the ensemble_size and num_epochs values should be adjusted depending on the available resources. # Train MAP ensemble model = model . fit ( df_train , seed = jax . random . PRNGKey ( 0 ), ensemble_size = 8 , num_epochs = 5000 , )","title":"Fitting the Estimator"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#plotting-training-loss","text":"Plotting training loss gives us a sense of convergence of the learning dynamics and agreement among differnet members of the ensemble. # Inspect the training loss for each particle. import matplotlib.pyplot as plt losses = np . row_stack ( model . losses_ ) fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . plot ( losses . T ) ax . plot ( np . mean ( losses , axis = 0 ), color = 'k' , linewidth = 3 ) ax . set_xlabel ( 'Epoch' ) ax . set_ylabel ( 'Negative Joint Probability' ) ax . set_yscale ( 'log' , base = 10 )","title":"Plotting Training Loss"},{"location":"tutorials/BayesNF_Tutorial_on_London_Air_Quality/#making-predictions","text":"The predict method takes in a test data frame, with the same format as the training data frame, except without the target column; quantiles, which are a list of numbers between 0 and 1. It returns mean predictions yhat and the requested quantiles yhat_quantiles . The yhat estimates are returned separately for each member of the ensemble whereas the yhat_quantiles estimates are computed across the entire ensemble. ! wget - q https : // cs . cmu . edu /~ fsaad / assets / bayesnf / air_quality .5 . test . csv df_test = pd . read_csv ( 'air_quality.5.test.csv' , index_col = 0 , parse_dates = [ 'datetime' ]) yhat , yhat_quantiles = model . predict ( df_test , quantiles = ( 0.025 , 0.5 , 0.975 )) It is helpful to show a scatter plot of the true vs predicted values on the test data. We will plot the median predictions yhat_quantiles[1] versus the true chickenpox value. fig , ax = plt . subplots ( figsize = ( 5 , 3 ), tight_layout = True ) ax . scatter ( df_test . pm10 , yhat_quantiles [ 1 ], marker = '.' , color = 'k' ) ax . plot ([ 0 , 250 ], [ 0 , 250 ], color = 'red' ) ax . set_xlabel ( 'True Value' ) ax . set_ylabel ( 'Predicted Value' ) Text(0, 0.5, 'Predicted Value') We can also show the forecats on the held-out data for each of the four counties in the test set. locations = df_test . location . unique () fig , axes = plt . subplots ( ncols = 4 , nrows = len ( locations ) // 4 , tight_layout = True , figsize = ( 16 , 10 )) for ax , location in zip ( axes . flat , locations ): y_train = df_train [ df_train . location == location ] y_test = df_test [ df_test . location == location ] ax . scatter ( y_train . datetime [ - 100 :], y_train . pm10 [ - 100 :], marker = 'o' , color = 'k' , label = 'Observations' ) ax . scatter ( y_test . datetime , y_test . pm10 , marker = 'o' , edgecolor = 'k' , facecolor = 'w' , label = 'Test Data' ) mask = df_test . location . to_numpy () == location ax . plot ( y_test . datetime , yhat_quantiles [ 1 ][ mask ], color = 'red' , label = 'Meidan Prediction' ) ax . fill_between ( y_test . datetime , yhat_quantiles [ 0 ][ mask ], yhat_quantiles [ 2 ][ mask ], alpha = 0.5 , label = '95% Prediction Interval' ) ax . set_title ( 'Test Location: %s ' % ( location ,)) ax . set_xticks ([]) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'PM10' ) axes . flat [ 0 ] . legend ( loc = 'upper left' ) <matplotlib.legend.Legend at 0x7f01f0413be0>","title":"Making Predictions"}]}